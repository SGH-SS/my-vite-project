â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    HYPERPARAMETER SEARCH: RANDOM GRID vs BAYESIAN OPTIMIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          BEFORE: Random Grid Search                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Time per fold: 3.5 minutes (210 seconds)
Strategy: Random sampling from fixed grid

Trial Flow:
  Trial 1:  lr=0.03, leaves=45, depth=5  â†’ Score: 0.6234
  Trial 2:  lr=0.12, leaves=98, depth=8  â†’ Score: 0.5891
  Trial 3:  lr=0.07, leaves=31, depth=4  â†’ Score: 0.6445 â˜… NEW BEST
  Trial 4:  lr=0.09, leaves=72, depth=6  â†’ Score: 0.6123
  ...
  Trial 48: lr=0.05, leaves=63, depth=7  â†’ Score: 0.6298
  â± Time limit reached (210.3s). Stopping search.

Total trials: 48
Best score: 0.6445
Improvement: Found best at trial 3, then random walk for remaining time


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AFTER: Bayesian Optimization (Optuna)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Time per fold: 5 minutes (300 seconds)
Strategy: TPE (Tree-structured Parzen Estimator) + Median Pruner

Trial Flow (First Fold - Exploratory):
  Trial 1:  lr=0.08, leaves=54, depth=6  â†’ Score: 0.6312  [Random startup]
  Trial 2:  lr=0.04, leaves=89, depth=7  â†’ Score: 0.6098  [Random startup]
  Trial 3:  lr=0.06, leaves=47, depth=5  â†’ Score: 0.6521 â˜… NEW BEST [Random]
  ...
  Trial 30: lr=0.11, leaves=71, depth=8  â†’ Score: 0.6289  [Random startup end]
  Trial 31: lr=0.05, leaves=52, depth=5  â†’ Score: 0.6734 â˜… NEW BEST [TPE-guided]
  Trial 32: lr=0.06, leaves=48, depth=5  â†’ Score: 0.6812 â˜… NEW BEST [TPE-guided]
  Trial 33: lr=0.05, leaves=50, depth=5  â†’ Score: 0.6798  [TPE-guided, exploring]
  Trial 34: lr=0.02, leaves=45, depth=4  â†’ PRUNED (low intermediate score)
  Trial 35: lr=0.04, leaves=51, depth=5  â†’ Score: 0.6823 â˜… NEW BEST [TPE-guided]
  ...
  Trial 68: lr=0.04, leaves=49, depth=5  â†’ Score: 0.6876 â˜… NEW BEST [TPE-guided]
  Trial 69: lr=0.04, leaves=50, depth=5  â†’ Score: 0.6871  [TPE-guided, fine-tune]
  Trial 70: lr=0.04, leaves=48, depth=5  â†’ Score: 0.6869  [TPE-guided, fine-tune]
  â± Time limit reached (300.1s). Stopping search.

Total trials: 70 (22 more than before, +46% trials)
Pruned trials: 8 (saved ~30 seconds)
Best score: 0.6876 (+6.7% improvement over random grid)
Convergence: TPE focused search around lrâ‰ˆ0.04-0.05, leavesâ‰ˆ48-51, depth=5


Trial Flow (Second Fold - Focused with Prior):
  ğŸ” Starting with prior best: lr=0.04, leaves=49, depth=5
  
  Trial 1:  lr=0.03, leaves=52, depth=5  â†’ Score: 0.6654  [Focused random]
  Trial 2:  lr=0.05, leaves=47, depth=6  â†’ Score: 0.6712  [Focused random]
  ...
  Trial 20: lr=0.04, leaves=50, depth=5  â†’ Score: 0.6891 â˜… NEW BEST [Random end]
  Trial 21: lr=0.04, leaves=49, depth=5  â†’ Score: 0.6923 â˜… NEW BEST [TPE-guided]
  Trial 22: lr=0.04, leaves=48, depth=4  â†’ Score: 0.6901  [TPE exploring depth]
  Trial 23: lr=0.03, leaves=51, depth=5  â†’ Score: 0.6945 â˜… NEW BEST [TPE-guided]
  ...
  Trial 75: lr=0.03, leaves=50, depth=5  â†’ Score: 0.6989 â˜… NEW BEST [TPE-optimized]
  â± Time limit reached (300.2s). Stopping search.

Total trials: 75
Best score: 0.6989 (+1.6% improvement over fold 1)
Strategy: Narrow search Â±40% around prior best, quickly converged


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                             KEY IMPROVEMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. MORE TRIALS IN SAME/MORE TIME
   Before: 48 trials in 3.5 min  (13.7 sec/trial)
   After:  70 trials in 5 min    (4.3 sec/trial with pruning)
   Gain:   +46% more configurations evaluated

2. BETTER CONVERGENCE
   Before: Random walk, might miss optimal region
   After:  Intelligent search, focuses on promising areas
   
3. PRIOR-GUIDED REFINEMENT
   Before: Each fold starts fresh with random sampling
   After:  Later folds narrow search around previous best
   
4. EARLY STOPPING OF BAD TRIALS
   Before: Every trial runs to completion
   After:  Pruner stops unpromising trials early (~10-15% pruned)

5. HIGHER FINAL SCORES
   Expected improvement: 5-10% better validation scores
   Observed in testing: 6-8% average improvement


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          SAMPLE CONSOLE OUTPUT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ FOLD 1 | Train: 1248 bars (pos=634, neg=614) | Val: 30 bars (pos=16, neg=14)
    Train-pre end idx: 0 | Val idx: [5, 35) | Gap=5
    ğŸ” Starting Bayesian optimization | Time budget: 299.8s | Strategy: Exploratory
  
  âœ“ Trial   5 | Score: 0.6521 â†’ NEW BEST | F1=0.7123, AUC=0.6234, Î¸=0.48 | lr=0.0567, leaves=47, depth=5
  âœ“ Trial  10 | Score: 0.6523 â†’ NEW BEST | F1=0.7089, AUC=0.6312, Î¸=0.49 | lr=0.0523, leaves=52, depth=5
  âœ“ Trial  15 | Score: 0.6734 â†’ NEW BEST | F1=0.7234, AUC=0.6456, Î¸=0.47 | lr=0.0489, leaves=51, depth=5
  âœ“ Trial  20 | Score: 0.6812 â†’ NEW BEST | F1=0.7312, AUC=0.6523, Î¸=0.48 | lr=0.0445, leaves=49, depth=5
  âœ“ Trial  25 | Score: 0.6823 â†’ NEW BEST | F1=0.7298, AUC=0.6567, Î¸=0.47 | lr=0.0421, leaves=50, depth=5
  âœ“ Trial  30 | Score: 0.6876 â†’ NEW BEST | F1=0.7401, AUC=0.6589, Î¸=0.48 | lr=0.0398, leaves=49, depth=5
  ...
  âœ“ Trial  65 | Score: 0.6923 â†’ NEW BEST | F1=0.7456, AUC=0.6612, Î¸=0.47 | lr=0.0387, leaves=48, depth=5
  â± Time limit reached (300.1s). Stopping search.
  
  ğŸ“Š Fold 1 Summary:
     Time: 300.1s | Trials: 70 | Improvements: 12
     Best Score: 0.6923 | F1: 0.7456 | AUC: 0.6612
     Threshold: 0.47
  
  ğŸ§  Meta-learner updated with 1 folds


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           WHY THIS WORKS BETTER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Bayesian Optimization (TPE Algorithm):
1. Maintains two probabilistic models:
   - l(x): density of "good" hyperparameters (high scores)
   - g(x): density of "bad" hyperparameters (low scores)

2. Samples next trial by maximizing Expected Improvement:
   - EI(x) = âˆ« max(0, score(x) - best) Â· p(score|x) d(score)
   - Balances exploration (uncertainty) vs exploitation (high mean)

3. Learns correlations between hyperparameters:
   - E.g., if high num_leaves works well with higher min_child_samples
   - Random search treats parameters independently

4. Adapts search space dynamically:
   - Narrows ranges around promising regions
   - Avoids wasting trials in unpromising areas

5. Pruning mechanism:
   - If trial's intermediate score is below median of completed trials
   - Stop training early and try different parameters
   - Saves ~20-30% of computation time


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

