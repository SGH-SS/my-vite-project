{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 1: CROSS-PLATFORM DEPENDENCY MANAGEMENT\n",
        "# ========================================\n",
        "print(\"🔧 Setting up dependencies...\")\n",
        "\n",
        "# Cross-platform dependency installation\n",
        "try:\n",
        "    import pandas, numpy, sklearn, xgboost, matplotlib, seaborn, joblib, tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import joblib\n",
        "    import lightgbm as lgb  # not used here, keeps env parity\n",
        "    print(\"✅ Core dependencies already available\")\n",
        "except ImportError as e:\n",
        "    print(f\"Installing missing dependencies: {e}\")\n",
        "    import sys, subprocess\n",
        "    pkgs = ['pandas', 'numpy', 'scikit-learn', 'xgboost', 'lightgbm',\n",
        "            'matplotlib', 'seaborn', 'joblib', 'tqdm', 'pyarrow']\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + pkgs)\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import joblib\n",
        "    print(\"✅ Dependencies installed\")\n",
        "\n",
        "# Try to mount Google Drive if available (Colab environment)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    BASE_DIR = '/content/drive/MyDrive/daygent_v1_models'  # same base folder as your LGBM 4h\n",
        "    print(\"✅ Google Drive mounted (Colab environment)\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    BASE_DIR = './daygent_v1_models'\n",
        "    print(\"✅ Local environment detected\")\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'spy_data_export')\n",
        "ORIGINAL_MODEL_DIR = os.path.join(BASE_DIR, 'gb_1d')  # Load from original location\n",
        "REVERSE_MODEL_DIR = os.path.join(BASE_DIR, 'gb_1d_reverse')  # Save reverse test results here\n",
        "os.makedirs(REVERSE_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"🔄 GB1D Model Reverse Test - Loading and Verifying Saved Model\")\n",
        "print(\"Target: Reproduce EXACT same results as original gb1d_iso.ipynb run\")\n",
        "print(\"Expected test accuracy: 0.7143, test AUC: 0.7733, threshold: 0.57\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✅ Original model directory: {ORIGINAL_MODEL_DIR}\")\n",
        "print(f\"✅ Reverse test output directory: {REVERSE_MODEL_DIR}\")\n",
        "print(f\"✅ Data directory: {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 2: LOAD SAVED MODEL ARTIFACTS\n",
        "# ========================================\n",
        "print(\"\\n🔧 Loading saved model artifacts...\")\n",
        "\n",
        "# Load configuration files\n",
        "with open(os.path.join(ORIGINAL_MODEL_DIR, 'results_gb_1d.json'), 'r') as f:\n",
        "    original_results = json.load(f)\n",
        "\n",
        "with open(os.path.join(ORIGINAL_MODEL_DIR, 'deployment_config.json'), 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Load model and scaler\n",
        "model_path = os.path.join(ORIGINAL_MODEL_DIR, 'gb_1d_final.joblib')\n",
        "scaler_path = os.path.join(ORIGINAL_MODEL_DIR, 'scaler_1d.joblib')\n",
        "\n",
        "gb_model = joblib.load(model_path)\n",
        "scaler = joblib.load(scaler_path)\n",
        "\n",
        "# Extract key parameters\n",
        "FEATURE_NAMES = config['feature_names']\n",
        "THRESHOLD = config['calibrated_threshold']\n",
        "TEST_PERIOD = original_results['test_period']\n",
        "\n",
        "print(f\"✅ Model loaded: {type(gb_model).__name__}\")\n",
        "print(f\"✅ Scaler loaded: {type(scaler).__name__}\")\n",
        "print(f\"📊 Feature count: {len(FEATURE_NAMES)}\")\n",
        "print(f\"🎯 Calibrated threshold: {THRESHOLD}\")\n",
        "print(f\"📅 Original test period: {TEST_PERIOD}\")\n",
        "\n",
        "# Load original predictions for comparison\n",
        "original_preds = pd.read_csv(os.path.join(ORIGINAL_MODEL_DIR, 'test_predictions_1d.csv'))\n",
        "print(f\"📋 Original predictions loaded: {len(original_preds)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 3: LOAD RAW DATA (EXACT SAME AS ORIGINAL)\n",
        "# ========================================\n",
        "print(\"\\n📊 Loading raw data (matching original process)...\")\n",
        "\n",
        "TIMEFRAMES_ORDERED = ['1d', '4h']\n",
        "raw_data = {}\n",
        "\n",
        "for tf in TIMEFRAMES_ORDERED:\n",
        "    csv_file = os.path.join(DATA_DIR, f'spy_{tf}.csv')\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise FileNotFoundError(f\"❌ {csv_file} not found!\")\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    raw_data[tf] = df\n",
        "    print(f\"✅ Loaded {tf} data: {len(df):,} candles\")\n",
        "    print(f\"📅 {tf} range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 4: RECREATE EXACT TEST PERIOD (MATCHING ORIGINAL LOGIC)\n",
        "# ========================================\n",
        "print(\"\\n🎯 Recreating exact test period determination...\")\n",
        "\n",
        "# Find common date range between 1d and 4h data\n",
        "latest_start = max(raw_data['1d']['timestamp'].min(), raw_data['4h']['timestamp'].min())\n",
        "earliest_end  = min(raw_data['1d']['timestamp'].max(), raw_data['4h']['timestamp'].max())\n",
        "\n",
        "# Find common trading days\n",
        "common_dates = set(raw_data['1d'][(raw_data['1d']['timestamp'] >= latest_start) &\n",
        "                                  (raw_data['1d']['timestamp'] <= earliest_end)]['timestamp'].dt.date.unique())\n",
        "common_dates &= set(raw_data['4h'][(raw_data['4h']['timestamp'] >= latest_start) &\n",
        "                                   (raw_data['4h']['timestamp'] <= earliest_end)]['timestamp'].dt.date.unique())\n",
        "\n",
        "# Select last 35 days (same as original)\n",
        "all_days = sorted(common_dates)\n",
        "TEST_DAYS = min(35, len(all_days))\n",
        "selected_days = all_days[-TEST_DAYS:]\n",
        "\n",
        "test_start = pd.Timestamp.combine(selected_days[0],  pd.Timestamp.min.time()).tz_localize('UTC')\n",
        "test_end   = pd.Timestamp.combine(selected_days[-1], pd.Timestamp.max.time()).tz_localize('UTC')\n",
        "\n",
        "print(f\"📅 Recreated test period: {test_start.date()} → {test_end.date()} ({TEST_DAYS} trading days)\")\n",
        "print(f\"🔍 Original test period: {TEST_PERIOD}\")\n",
        "\n",
        "# Verify we got the exact same period\n",
        "expected_start = \"2024-12-17\"\n",
        "expected_end = \"2025-02-07\"\n",
        "if str(test_start.date()) == expected_start and str(test_end.date()) == expected_end:\n",
        "    print(\"✅ Test period matches original exactly!\")\n",
        "else:\n",
        "    print(f\"⚠️  Test period mismatch! Expected: {expected_start} to {expected_end}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 5: FEATURE EXTRACTION FUNCTIONS (EXACT SAME AS ORIGINAL)\n",
        "# ========================================\n",
        "print(\"\\n🔧 Setting up feature extraction (exact same as original)...\")\n",
        "\n",
        "def parse_vector_column(vector_str):\n",
        "    \"\"\"Parse vector string to numpy array\"\"\"\n",
        "    if pd.isna(vector_str) or vector_str is None:\n",
        "        return None\n",
        "    if isinstance(vector_str, str):\n",
        "        s = vector_str.strip('[]\"')\n",
        "        try:\n",
        "            return np.array([float(x.strip()) for x in s.split(',')])\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return np.array(vector_str)\n",
        "\n",
        "def build_feature_vector(raw_ohlcv, iso_ohlc, tf, tf_list):\n",
        "    \"\"\"Build 16-feature vector\"\"\"\n",
        "    o, h, l, c, v = raw_ohlcv\n",
        "    features = list(raw_ohlcv)          # 5\n",
        "    features.extend(list(iso_ohlc))     # 4\n",
        "    features.extend([1 if tf == t else 0 for t in tf_list])  # 2\n",
        "    features.extend([\n",
        "        (h - l) / c if c else 0,        # hl_range\n",
        "        (c - o) / o if o else 0,        # price_change\n",
        "        (h - c) / c if c else 0,        # upper_shadow\n",
        "        (c - l) / c if c else 0,        # lower_shadow\n",
        "        v / 1_000_000,                  # volume_m\n",
        "    ])  # 5\n",
        "    return np.array(features, dtype=float)\n",
        "\n",
        "def extract_features_1d(row):\n",
        "    raw_ohlcv = parse_vector_column(row.get('raw_ohlcv_vec'))\n",
        "    iso_ohlc  = parse_vector_column(row.get('iso_ohlc'))\n",
        "    future    = row.get('future')\n",
        "    if raw_ohlcv is None or iso_ohlc is None or pd.isna(future):\n",
        "        return None, None\n",
        "    if len(raw_ohlcv) != 5 or len(iso_ohlc) != 4:\n",
        "        return None, None\n",
        "    return build_feature_vector(raw_ohlcv, iso_ohlc, '1d', TIMEFRAMES_ORDERED), int(future)\n",
        "\n",
        "print(f\"✅ Feature extraction functions ready\")\n",
        "print(f\"📋 Expected feature names: {FEATURE_NAMES}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 6: EXTRACT TEST FEATURES (EXACT SAME LOGIC)\n",
        "# ========================================\n",
        "print(\"\\n🔄 Extracting test features (matching original process)...\")\n",
        "\n",
        "df_1d = raw_data['1d']\n",
        "test_df = df_1d[(df_1d['timestamp'] >= test_start) & (df_1d['timestamp'] <= test_end)].copy()\n",
        "\n",
        "print(f\"📊 Test samples from data: {len(test_df)}\")\n",
        "print(f\"📊 Expected test samples: {original_results['test_samples']}\")\n",
        "\n",
        "# Extract test features and store detailed info\n",
        "X_test, y_test, test_timestamps = [], [], []\n",
        "test_rows_info = []\n",
        "\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Extracting test features\"):\n",
        "    fv, lbl = extract_features_1d(row)\n",
        "    if fv is not None:\n",
        "        X_test.append(fv)\n",
        "        y_test.append(lbl)\n",
        "        test_timestamps.append(row['timestamp'])\n",
        "        test_rows_info.append({\n",
        "            'timestamp': row['timestamp'],\n",
        "            'raw_ohlcv': parse_vector_column(row['raw_ohlcv_vec']),\n",
        "            'iso_ohlc':  parse_vector_column(row['iso_ohlc']),\n",
        "            'future': int(row['future']),\n",
        "            'feature_vector': fv\n",
        "        })\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"📊 Extracted test features: {X_test.shape}\")\n",
        "print(f\"📊 Test labels: {len(y_test)}\")\n",
        "\n",
        "# Verify we got exactly the same number of samples\n",
        "if len(X_test) == original_results['test_samples']:\n",
        "    print(\"✅ Test sample count matches original exactly!\")\n",
        "else:\n",
        "    print(f\"⚠️  Sample count mismatch! Got {len(X_test)}, expected {original_results['test_samples']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 7: MAKE PREDICTIONS WITH LOADED MODEL\n",
        "# ========================================\n",
        "print(\"\\n🧠 Making predictions with loaded model...\")\n",
        "\n",
        "# Scale test features with loaded scaler (same as original)\n",
        "X_test_scaled = scaler.transform(X_test) if len(X_test) else np.empty((0, len(FEATURE_NAMES)))\n",
        "\n",
        "# Make predictions\n",
        "test_pred_proba = gb_model.predict_proba(X_test_scaled)[:, 1] if len(X_test_scaled) else np.array([])\n",
        "test_pred = (test_pred_proba >= THRESHOLD).astype(int) if len(test_pred_proba) else np.array([])\n",
        "\n",
        "# Calculate metrics\n",
        "test_acc = accuracy_score(y_test, test_pred) if len(test_pred) else float('nan')\n",
        "test_auc = roc_auc_score(y_test, test_pred_proba) if (len(test_pred_proba) and len(np.unique(y_test))==2) else float('nan')\n",
        "\n",
        "print(f\"\\n🎯 REVERSE TEST RESULTS:\")\n",
        "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"✅ Test AUC: {test_auc:.4f}\")\n",
        "print(f\"📊 Threshold used: {THRESHOLD}\")\n",
        "if len(test_pred):\n",
        "    print(f\"📊 Test predictions: {np.bincount(test_pred)}\")\n",
        "    print(f\"📊 Actual labels: {np.bincount(y_test)}\")\n",
        "\n",
        "print(f\"\\n🔍 COMPARISON WITH ORIGINAL:\")\n",
        "print(f\"Original Test Accuracy: {original_results['test_accuracy']:.4f}\")\n",
        "print(f\"Original Test AUC: {original_results['test_auc']:.4f}\")\n",
        "print(f\"Original Threshold: {original_results['chosen_threshold']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 8: DETAILED PREDICTION-BY-PREDICTION COMPARISON\n",
        "# ========================================\n",
        "print(\"\\n🔍 Detailed prediction-by-prediction comparison...\")\n",
        "\n",
        "# Build new predictions table\n",
        "new_records = []\n",
        "for i, info in enumerate(test_rows_info):\n",
        "    ts   = info['timestamp']\n",
        "    fv   = info['feature_vector']\n",
        "    raw  = info['raw_ohlcv']\n",
        "    iso  = info['iso_ohlc']\n",
        "    true = info['future']\n",
        "\n",
        "    proba = float(test_pred_proba[i])\n",
        "    pred  = int(test_pred[i])\n",
        "    correct = bool(pred == true)\n",
        "    margin = proba - THRESHOLD\n",
        "\n",
        "    rec = {\n",
        "        'candle_index_in_test': i + 1,\n",
        "        'timestamp_utc': ts,\n",
        "        'date_utc': ts.date(),\n",
        "        'pred_prob_up': proba,\n",
        "        'pred_label': int(pred),\n",
        "        'true_label': int(true),\n",
        "        'correct': correct,\n",
        "        'threshold_used': THRESHOLD,\n",
        "        'decision_margin': margin,\n",
        "        'raw_o': raw[0], 'raw_h': raw[1], 'raw_l': raw[2], 'raw_c': raw[3], 'raw_v': raw[4],\n",
        "        'iso_0': iso[0], 'iso_1': iso[1], 'iso_2': iso[2], 'iso_3': iso[3],\n",
        "        'tf_1d': fv[FEATURE_NAMES.index('tf_1d')],\n",
        "        'tf_4h': fv[FEATURE_NAMES.index('tf_4h')],\n",
        "        'hl_range': fv[FEATURE_NAMES.index('hl_range')],\n",
        "        'price_change': fv[FEATURE_NAMES.index('price_change')],\n",
        "        'upper_shadow': fv[FEATURE_NAMES.index('upper_shadow')],\n",
        "        'lower_shadow': fv[FEATURE_NAMES.index('lower_shadow')],\n",
        "        'volume_m': fv[FEATURE_NAMES.index('volume_m')],\n",
        "    }\n",
        "    new_records.append(rec)\n",
        "\n",
        "new_pred_df = pd.DataFrame.from_records(new_records).sort_values(['date_utc','timestamp_utc']).reset_index(drop=True)\n",
        "\n",
        "print(f\"📊 New predictions table: {len(new_pred_df)} rows\")\n",
        "print(f\"📊 Original predictions table: {len(original_preds)} rows\")\n",
        "\n",
        "# Compare key columns\n",
        "comparison_cols = ['pred_prob_up', 'pred_label', 'true_label', 'correct']\n",
        "print(\"\\n🔍 Comparing key prediction columns:\")\n",
        "\n",
        "all_match = True\n",
        "for col in comparison_cols:\n",
        "    if col in original_preds.columns and col in new_pred_df.columns:\n",
        "        if col == 'pred_prob_up':\n",
        "            # For probabilities, allow small floating point differences\n",
        "            diff = np.abs(original_preds[col].values - new_pred_df[col].values)\n",
        "            max_diff = np.max(diff)\n",
        "            matches = np.allclose(original_preds[col].values, new_pred_df[col].values, rtol=1e-10, atol=1e-10)\n",
        "            print(f\"  {col}: {'✅ EXACT MATCH' if matches else '❌ MISMATCH'} (max diff: {max_diff:.2e})\")\n",
        "            if not matches:\n",
        "                all_match = False\n",
        "        else:\n",
        "            matches = (original_preds[col].values == new_pred_df[col].values).all()\n",
        "            print(f\"  {col}: {'✅ EXACT MATCH' if matches else '❌ MISMATCH'}\")\n",
        "            if not matches:\n",
        "                all_match = False\n",
        "    else:\n",
        "        print(f\"  {col}: ⚠️  Column not found in one of the datasets\")\n",
        "        all_match = False\n",
        "\n",
        "print(f\"\\n🎯 OVERALL COMPARISON: {'✅ ALL PREDICTIONS MATCH EXACTLY!' if all_match else '❌ SOME DIFFERENCES FOUND'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 9: FINAL VALIDATION SUMMARY\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL VALIDATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Metrics comparison\n",
        "acc_match = abs(test_acc - original_results['test_accuracy']) < 1e-10\n",
        "auc_match = abs(test_auc - original_results['test_auc']) < 1e-10\n",
        "threshold_match = abs(THRESHOLD - original_results['chosen_threshold']) < 1e-10\n",
        "\n",
        "print(f\"📊 Test Accuracy Match: {'✅' if acc_match else '❌'} ({test_acc:.6f} vs {original_results['test_accuracy']:.6f})\")\n",
        "print(f\"📊 Test AUC Match: {'✅' if auc_match else '❌'} ({test_auc:.6f} vs {original_results['test_auc']:.6f})\")\n",
        "print(f\"📊 Threshold Match: {'✅' if threshold_match else '❌'} ({THRESHOLD} vs {original_results['chosen_threshold']})\")\n",
        "print(f\"📊 Sample Count Match: {'✅' if len(X_test) == original_results['test_samples'] else '❌'} ({len(X_test)} vs {original_results['test_samples']})\")\n",
        "print(f\"📊 Prediction Details Match: {'✅' if all_match else '❌'}\")\n",
        "\n",
        "# Overall validation\n",
        "perfect_match = acc_match and auc_match and threshold_match and len(X_test) == original_results['test_samples'] and all_match\n",
        "\n",
        "print(f\"\\n🎯 REVERSE TEST RESULT: {'🎉 PERFECT MATCH! Model loaded and reproduced identical results.' if perfect_match else '⚠️  Some differences detected. Review above for details.'}\")\n",
        "\n",
        "if perfect_match:\n",
        "    print(\"\\n✅ The saved GB1D model has been successfully validated!\")\n",
        "    print(\"✅ All predictions, metrics, and results match the original training run exactly.\")\n",
        "    print(\"✅ The model can be confidently deployed for production use.\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Validation failed. The loaded model does not reproduce identical results.\")\n",
        "    print(\"⚠️  This could indicate:\")\n",
        "    print(\"   - Data loading differences\")\n",
        "    print(\"   - Feature extraction differences\")\n",
        "    print(\"   - Model/scaler loading issues\")\n",
        "    print(\"   - Random seed or environment differences\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 10: SAVE REVERSE TEST RESULTS\n",
        "# ========================================\n",
        "print(\"\\n💾 Saving reverse test results...\")\n",
        "\n",
        "reverse_test_results = {\n",
        "    'reverse_test_date': datetime.now().isoformat(),\n",
        "    'perfect_match': perfect_match,\n",
        "    'loaded_model_results': {\n",
        "        'test_accuracy': float(test_acc),\n",
        "        'test_auc': float(test_auc),\n",
        "        'threshold_used': float(THRESHOLD),\n",
        "        'test_samples': int(len(X_test)),\n",
        "        'predictions_match': all_match\n",
        "    },\n",
        "    'original_results': original_results,\n",
        "    'differences': {\n",
        "        'accuracy_diff': float(abs(test_acc - original_results['test_accuracy'])),\n",
        "        'auc_diff': float(abs(test_auc - original_results['test_auc'])),\n",
        "        'sample_count_match': len(X_test) == original_results['test_samples']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results to the reverse test directory\n",
        "results_path = os.path.join(REVERSE_MODEL_DIR, 'gb1d_reverse_test_results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(reverse_test_results, f, indent=2)\n",
        "\n",
        "# Save the new predictions for manual inspection if needed\n",
        "preds_path = os.path.join(REVERSE_MODEL_DIR, 'gb1d_reverse_test_predictions.csv')\n",
        "new_pred_df.to_csv(preds_path, index=False)\n",
        "\n",
        "print(f\"✅ Reverse test results saved to: {results_path}\")\n",
        "print(f\"✅ New predictions saved to: {preds_path}\")\n",
        "print(\"\\n🏁 Reverse test complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
