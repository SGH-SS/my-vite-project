{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 1: CROSS-PLATFORM SETUP AND PATHS\n",
        "print(\"\ud83d\udd27 Setting up environment...\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try optional Colab drive mount for convenience\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    BASE_DIR = '/content/drive/MyDrive/daygent_v1_models'\n",
        "    print(\"\u2705 Google Drive mounted (Colab)\")\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "    BASE_DIR = './daygent_v1_models'\n",
        "    print(\"\u2705 Local environment detected\")\n",
        "\n",
        "# Core imports (install if missing)\n",
        "try:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "    import joblib\n",
        "except Exception as e:\n",
        "    print(f\"Installing missing packages due to: {e}\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'numpy', 'pandas', 'scikit-learn', 'tqdm', 'joblib', 'pyarrow', 'matplotlib', 'lightgbm'])\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "    import joblib\n",
        "\n",
        "# Locations\n",
        "FRONTTEST_DIR = os.path.join(BASE_DIR, 'spy_data_fronttest')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'gb_4h')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'gb_4h_reverse_fronttest')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udd04 GB4H Reverse Test on Fronttest SPY 4H\")\n",
        "print(f\"\ud83d\udcc1 Fronttest dir: {FRONTTEST_DIR}\")\n",
        "print(f\"\ud83d\udcc1 Model dir:     {MODEL_DIR}\")\n",
        "print(f\"\ud83d\udcc1 Output dir:    {OUTPUT_DIR}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 2: LOAD MODEL CONFIG + ARTIFACTS\n",
        "print(\"\\n\ud83d\udce6 Loading model config and artifacts...\")\n",
        "\n",
        "def first_existing(path: str, candidates):\n",
        "    for name in candidates:\n",
        "        p = os.path.join(path, name)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "config_path = first_existing(MODEL_DIR, ['deployment_config_4h.json', 'deployment_config.json'])\n",
        "results_path = first_existing(MODEL_DIR, ['results_gb_4h_w2_style.json', 'results_gb_4h.json', 'results_4h_only.json'])\n",
        "model_path = first_existing(MODEL_DIR, ['gb_4h_w2_style.joblib', 'gb_4h_final.joblib', 'gb_4h.joblib'])\n",
        "scaler_path = first_existing(MODEL_DIR, ['scaler_4h_w2_style.joblib', 'scaler_4h.joblib', 'scaler_4h_only.joblib'])\n",
        "\n",
        "if not all([config_path, model_path, scaler_path]):\n",
        "    raise FileNotFoundError(f\"Missing required artifacts in {MODEL_DIR}. Found: config={config_path}, model={model_path}, scaler={scaler_path}\")\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    CONFIG = json.load(f)\n",
        "\n",
        "ORIGINAL_RESULTS = None\n",
        "if results_path and os.path.exists(results_path):\n",
        "    try:\n",
        "        with open(results_path, 'r') as f:\n",
        "            ORIGINAL_RESULTS = json.load(f)\n",
        "    except Exception:\n",
        "        ORIGINAL_RESULTS = None\n",
        "\n",
        "FEATURE_NAMES = CONFIG.get('feature_names') or CONFIG.get('features')\n",
        "THRESHOLD = float(CONFIG.get('calibrated_threshold', 0.5))\n",
        "\n",
        "gb_model = joblib.load(model_path)\n",
        "scaler = joblib.load(scaler_path)\n",
        "\n",
        "print(f\"\u2705 Model loaded: {type(gb_model).__name__}\")\n",
        "print(f\"\u2705 Scaler loaded: {type(scaler).__name__}\")\n",
        "if FEATURE_NAMES:\n",
        "    print(f\"\ud83d\udcca Feature contract: {len(FEATURE_NAMES)} features -> {FEATURE_NAMES}\")\n",
        "else:\n",
        "    print(\"\ud83d\udcca Feature contract: unknown (no feature_names in config)\")\n",
        "print(f\"\ud83c\udfaf Threshold: {THRESHOLD}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 3: LOAD FRONTTEST CSV (4h)\n",
        "print(\"\\n\ud83d\udce5 Loading fronttest CSVs...\")\n",
        "\n",
        "def find_fronttest_csv(base_dir: str, preferred_names, prefix: str):\n",
        "    for name in preferred_names:\n",
        "        p = os.path.join(base_dir, name)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    # Fallback: search by prefix\n",
        "    try:\n",
        "        for fname in os.listdir(base_dir):\n",
        "            if fname.lower().startswith(prefix) and fname.lower().endswith('.csv'):\n",
        "                return os.path.join(base_dir, fname)\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "csv_4h = find_fronttest_csv(\n",
        "    FRONTTEST_DIR,\n",
        "    ['fronttest_spy_4h.csv', 'fronttest_spy_4h_only.csv', 'fronttest_spy_4h_w2_style.csv'],\n",
        "    'fronttest_spy_4h'\n",
        ")\n",
        "\n",
        "if not csv_4h or not os.path.exists(csv_4h):\n",
        "    raise FileNotFoundError(f\"fronttest_spy_4h*.csv not found in {FRONTTEST_DIR}\")\n",
        "\n",
        "df_4h = pd.read_csv(csv_4h)\n",
        "if 'timestamp' not in df_4h.columns:\n",
        "    raise ValueError(\"4h fronttest CSV must include 'timestamp'\")\n",
        "\n",
        "df_4h['timestamp'] = pd.to_datetime(df_4h['timestamp'])\n",
        "df_4h = df_4h.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "print(f\"\u2705 4h rows: {len(df_4h):,}; range: {df_4h['timestamp'].min()} \u2192 {df_4h['timestamp'].max()}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 4: FEATURE EXTRACTION HELPERS (MATCH gb_4h CONTRACT)\n",
        "print(\"\\n\ud83d\udd27 Preparing feature helpers (gb_4h 16-feature contract)...\")\n",
        "\n",
        "TIMEFRAMES = ['1d', '4h']  # one-hot order used originally\n",
        "\n",
        "def parse_vector_column(vector_str):\n",
        "    if pd.isna(vector_str) or vector_str is None:\n",
        "        return None\n",
        "    if isinstance(vector_str, str):\n",
        "        s = vector_str.strip('[]\"')\n",
        "        try:\n",
        "            return np.array([float(x.strip()) for x in s.split(',')])\n",
        "        except Exception:\n",
        "            return None\n",
        "    return np.array(vector_str)\n",
        "\n",
        "def build_feature_vector(raw_ohlcv, iso_ohlc, tf, tf_list):\n",
        "    o, h, l, c, v = raw_ohlcv\n",
        "    feats = list(raw_ohlcv)\n",
        "    feats.extend(list(iso_ohlc))\n",
        "    feats.extend([1 if tf == t else 0 for t in tf_list])\n",
        "    feats.extend([\n",
        "        (h - l) / c if c else 0.0,           # hl_range\n",
        "        (c - o) / o if o else 0.0,           # price_change\n",
        "        (h - c) / c if c else 0.0,           # upper_shadow\n",
        "        (c - l) / c if c else 0.0,           # lower_shadow\n",
        "        (v / 1_000_000.0) if v is not None else 0.0  # volume_m\n",
        "    ])\n",
        "    return np.array(feats, dtype=float)\n",
        "\n",
        "def row_to_features_and_label(row):\n",
        "    raw_ohlcv = parse_vector_column(row.get('raw_ohlcv_vec'))\n",
        "    iso_ohlc  = parse_vector_column(row.get('iso_ohlc'))\n",
        "    future    = row.get('future')\n",
        "    if raw_ohlcv is None or iso_ohlc is None or pd.isna(future):\n",
        "        return None, None\n",
        "    return build_feature_vector(raw_ohlcv, iso_ohlc, '4h', TIMEFRAMES), int(future)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 5: BUILD TEST MATRIX FROM FRONTTEST\n",
        "print(\"\\n\ud83d\udcd0 Building test matrices from fronttest data...\")\n",
        "\n",
        "X_test, y_test, meta = [], [], []\n",
        "for idx, row in tqdm(df_4h.iterrows(), total=len(df_4h), desc='Fronttest rows'):\n",
        "    feats, label = row_to_features_and_label(row)\n",
        "    if feats is None:\n",
        "        continue\n",
        "    X_test.append(feats)\n",
        "    y_test.append(label)\n",
        "    meta.append({\n",
        "        'index': int(idx),\n",
        "        'timestamp': row['timestamp'],\n",
        "        'close': row.get('close', np.nan),\n",
        "        'future': int(label)\n",
        "    })\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "print(f\"\u2705 Test matrix: {X_test.shape} (labels: {np.bincount(y_test) if len(y_test) else '[]'})\")\n",
        "\n",
        "if X_test.size == 0:\n",
        "    raise RuntimeError(\"No valid rows in fronttest CSV with required vectors + future label.\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 6: SCALE, PREDICT, METRICS\n",
        "print(\"\\n\ud83e\uddea Inference + metrics...\")\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "pred = (proba >= THRESHOLD).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test, pred)\n",
        "auc = roc_auc_score(y_test, proba) if len(np.unique(y_test)) == 2 else float('nan')\n",
        "print(f\"\ud83c\udfaf Accuracy: {acc:.4f}\")\n",
        "print(f\"\ud83c\udfaf AUC:      {auc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "print(\"\\n\ud83d\udcca Confusion Matrix:\\n\", cm)\n",
        "print(\"\\n\ud83d\udccb Classification Report:\\n\", classification_report(y_test, pred, digits=4))\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 7: DAY-BY-DAY ANALYSIS\n",
        "print(\"\\n\ud83d\udcc5 Day-by-day breakdown...\")\n",
        "\n",
        "results_rows = []\n",
        "for i, info in enumerate(meta):\n",
        "    ts = pd.Timestamp(info['timestamp'])\n",
        "    date_key = ts.date()\n",
        "    results_rows.append({\n",
        "        'date': str(date_key),\n",
        "        'timestamp': ts.isoformat(),\n",
        "        'close': float(info['close']) if info['close'] is not None and not pd.isna(info['close']) else None,\n",
        "        'future': int(info['future']),\n",
        "        'prob_up': float(proba[i]),\n",
        "        'pred': int(pred[i])\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results_rows)\n",
        "\n",
        "daily = df_results.groupby('date').apply(lambda g: pd.Series({\n",
        "    'n': len(g),\n",
        "    'acc': float((g['pred'] == g['future']).mean()),\n",
        "    'avg_prob_up': float(g['prob_up'].mean()),\n",
        "    'pred_up_rate': float((g['pred'] == 1).mean()),\n",
        "    'true_up_rate': float((g['future'] == 1).mean())\n",
        "})).reset_index()\n",
        "\n",
        "summary = {\n",
        "    'overall': {\n",
        "        'n_samples': int(len(df_results)),\n",
        "        'accuracy': float(acc),\n",
        "        'auc': float(auc),\n",
        "        'threshold': float(THRESHOLD)\n",
        "    },\n",
        "    'by_day': daily.to_dict(orient='records')\n",
        "}\n",
        "\n",
        "summary_path = os.path.join(OUTPUT_DIR, 'fronttest_summary_gb4h.json')\n",
        "preds_csv = os.path.join(OUTPUT_DIR, 'fronttest_predictions_gb4h.csv')\n",
        "daily_csv = os.path.join(OUTPUT_DIR, 'fronttest_daily_metrics_gb4h.csv')\n",
        "\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "df_results.to_csv(preds_csv, index=False)\n",
        "daily.to_csv(daily_csv, index=False)\n",
        "\n",
        "print(f\"\\n\u2705 Saved: {summary_path}\")\n",
        "print(f\"\u2705 Saved: {preds_csv}\")\n",
        "print(f\"\u2705 Saved: {daily_csv}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 8: DISPLAY TOP/BOTTOM DAYS\n",
        "print(\"\\n\ud83c\udfc1 Best/Worst days by accuracy (>=3 samples/day)...\")\n",
        "\n",
        "eligible = daily[daily['n'] >= 3].copy()\n",
        "if len(eligible):\n",
        "    print(\"Top 5 days:\")\n",
        "    print(eligible.sort_values('acc', ascending=False).head(5))\n",
        "    print(\"\\nBottom 5 days:\")\n",
        "    print(eligible.sort_values('acc', ascending=True).head(5))\n",
        "else:\n",
        "    print(\"Not enough samples per-day for breakdown; showing head:\")\n",
        "    print(daily.head())\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 9: VISUALIZE DAILY ACCURACY\n",
        "print(\"\\n\ud83d\udcc8 Visualizing daily accuracy...\")\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "    if len(daily):\n",
        "        dplot = daily.copy()\n",
        "        dplot['date_dt'] = pd.to_datetime(dplot['date'])\n",
        "        dplot = dplot.sort_values('date_dt')\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 4.5), dpi=140)\n",
        "        sizes = 20 + 3.0 * dplot['n'].astype(float)\n",
        "        sc = ax.scatter(\n",
        "            dplot['date_dt'], dplot['acc'],\n",
        "            c=dplot['n'], cmap='viridis', s=sizes,\n",
        "            alpha=0.9, edgecolor='k', linewidth=0.3, label='Daily accuracy'\n",
        "        )\n",
        "        ax.plot(dplot['date_dt'], dplot['acc'], color='gray', alpha=0.35, linewidth=1)\n",
        "        ax.axhline(acc, color='#1f77b4', linestyle='--', linewidth=1.5, label=f'Overall: {acc:.2%}')\n",
        "        ax.set_title('Daily Accuracy (4h)', fontsize=13)\n",
        "        ax.set_xlabel('Date'); ax.set_ylabel('Accuracy'); ax.set_ylim(0, 1)\n",
        "        ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
        "        locator = mdates.AutoDateLocator(minticks=6, maxticks=10)\n",
        "        ax.xaxis.set_major_locator(locator)\n",
        "        ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator))\n",
        "        ax.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax.legend(loc='lower left')\n",
        "        cbar = plt.colorbar(sc, ax=ax, pad=0.015); cbar.set_label('Samples per day (n)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        out_plot = os.path.join(OUTPUT_DIR, 'fronttest_daily_accuracy_gb4h.png')\n",
        "        plt.savefig(out_plot, bbox_inches='tight'); print(f\"\u2705 Saved plot to {out_plot}\")\n",
        "        plt.close(fig)\n",
        "except Exception as e:\n",
        "    print(f\"Plotting skipped due to: {e}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [cell] STEP 10: PERIOD-SPECIFIC ANALYSIS (Apr 25 \u2192 Jun 13)\n",
        "print(\"\\n\ud83d\udd0e Period analysis: Apr 25 \u2192 Jun 13 (deep dive)\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, average_precision_score,\n",
        "    balanced_accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Safety checks\n",
        "required_cols = {'date', 'timestamp', 'future', 'prob_up'}\n",
        "missing_cols = required_cols - set(df_results.columns)\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"df_results is missing columns: {missing_cols}\")\n",
        "\n",
        "# Normalize dates and infer year\n",
        "dfp = df_results.copy()\n",
        "dfp['date_dt'] = pd.to_datetime(dfp['date'])\n",
        "dfp['timestamp_dt'] = pd.to_datetime(dfp['timestamp'], utc=True).dt.tz_localize(None)\n",
        "latest_year = int(dfp['date_dt'].dt.year.max())\n",
        "\n",
        "# Define window (inclusive)\n",
        "start_date = pd.Timestamp(latest_year, 4, 25)\n",
        "end_date = pd.Timestamp(latest_year, 6, 13)\n",
        "mask = (dfp['date_dt'] >= start_date) & (dfp['date_dt'] <= end_date)\n",
        "dfw = dfp.loc[mask].sort_values('timestamp_dt').reset_index(drop=True)\n",
        "\n",
        "if dfw.empty:\n",
        "    print(f\"\u26a0\ufe0f No samples between {start_date.date()} and {end_date.date()} (year inferred: {latest_year}).\")\n",
        "else:\n",
        "    y_true = dfw['future'].astype(int).to_numpy()\n",
        "    p_up = dfw['prob_up'].astype(float).to_numpy()\n",
        "    pred_w = (p_up >= THRESHOLD).astype(int)\n",
        "\n",
        "    # Core metrics\n",
        "    acc_w = accuracy_score(y_true, pred_w)\n",
        "    bal_acc_w = balanced_accuracy_score(y_true, pred_w)\n",
        "    auc_w = roc_auc_score(y_true, p_up) if len(np.unique(y_true)) == 2 else float('nan')\n",
        "    ap_w = average_precision_score(y_true, p_up) if len(np.unique(y_true)) == 2 else float('nan')\n",
        "    brier_w = float(np.mean((p_up - y_true) ** 2))\n",
        "    cm_w = confusion_matrix(y_true, pred_w)\n",
        "\n",
        "    # Per-day summary within window\n",
        "    daily_w = dfw.groupby('date_dt').apply(lambda g: pd.Series({\n",
        "        'n': len(g),\n",
        "        'acc': float((g['prob_up'].ge(THRESHOLD).astype(int) == g['future']).mean()),\n",
        "        'avg_prob_up': float(g['prob_up'].mean()),\n",
        "        'pred_up_rate': float((g['prob_up'].ge(THRESHOLD)).mean()),\n",
        "        'true_up_rate': float((g['future'] == 1).mean())\n",
        "    })).reset_index().sort_values('date_dt')\n",
        "\n",
        "    # Threshold sweep for sensitivity\n",
        "    def evaluate_thresholds(y, p, thresholds):\n",
        "        rows = []\n",
        "        for t in thresholds:\n",
        "            pred_t = (p >= t).astype(int)\n",
        "            acc_t = accuracy_score(y, pred_t)\n",
        "            bal_acc_t = balanced_accuracy_score(y, pred_t)\n",
        "            prec, rec, f1, _ = precision_recall_fscore_support(y, pred_t, average='binary', zero_division=0)\n",
        "            pos_rate = float((pred_t == 1).mean())\n",
        "            rows.append({\n",
        "                'threshold': float(t),\n",
        "                'accuracy': float(acc_t),\n",
        "                'balanced_accuracy': float(bal_acc_t),\n",
        "                'precision': float(prec),\n",
        "                'recall': float(rec),\n",
        "                'f1': float(f1),\n",
        "                'pred_up_rate': float(pos_rate)\n",
        "            })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    sweep = evaluate_thresholds(y_true, p_up, np.round(np.arange(0.30, 0.701, 0.025), 3))\n",
        "    best_by_acc = sweep.sort_values('accuracy', ascending=False).head(3).reset_index(drop=True)\n",
        "    best_by_f1 = sweep.sort_values('f1', ascending=False).head(3).reset_index(drop=True)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\ud83d\udcc5 Window: {start_date.date()} \u2192 {end_date.date()} (year inferred: {latest_year})\")\n",
        "    print(f\"\ud83e\uddee Samples: {len(dfw):,} | Days: {dfw['date_dt'].nunique()}\")\n",
        "    print(f\"\ud83c\udfaf Accuracy: {acc_w:.4f} | Balanced Acc: {bal_acc_w:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 ROC-AUC: {auc_w:.4f} | PR-AUC: {ap_w:.4f}\")\n",
        "    print(f\"\ud83c\udfaf Threshold used: {THRESHOLD:.3f} | Brier score: {brier_w:.5f}\")\n",
        "    print(\"\\n\ud83d\udcca Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm_w)\n",
        "    print(\"\\n\ud83d\udccb Classification Report:\")\n",
        "    print(classification_report(y_true, pred_w, digits=4, zero_division=0))\n",
        "\n",
        "    # Save artifacts\n",
        "    period_tag = f\"{start_date.date()}_to_{end_date.date()}\"\n",
        "    out_summary = {\n",
        "        'period': {'start': str(start_date.date()), 'end': str(end_date.date())},\n",
        "        'n_samples': int(len(dfw)),\n",
        "        'n_days': int(dfw['date_dt'].nunique()),\n",
        "        'threshold': float(THRESHOLD),\n",
        "        'metrics': {\n",
        "            'accuracy': float(acc_w),\n",
        "            'balanced_accuracy': float(bal_acc_w),\n",
        "            'roc_auc': float(auc_w),\n",
        "            'pr_auc': float(ap_w),\n",
        "            'brier': float(brier_w)\n",
        "        },\n",
        "        'confusion_matrix': cm_w.tolist()\n",
        "    }\n",
        "    summary_json = os.path.join(OUTPUT_DIR, f'period_summary_{period_tag}.json')\n",
        "    sweep_csv = os.path.join(OUTPUT_DIR, f'period_threshold_sweep_{period_tag}.csv')\n",
        "    preds_csv = os.path.join(OUTPUT_DIR, f'period_predictions_{period_tag}.csv')\n",
        "    daily_csv_w = os.path.join(OUTPUT_DIR, f'period_daily_metrics_{period_tag}.csv')\n",
        "    with open(summary_json, 'w') as f:\n",
        "        json.dump(out_summary, f, indent=2)\n",
        "    sweep.to_csv(sweep_csv, index=False)\n",
        "    dfw[['date', 'timestamp', 'close', 'future', 'prob_up']].to_csv(preds_csv, index=False)\n",
        "    daily_w.to_csv(daily_csv_w, index=False)\n",
        "    print(f\"\\n\u2705 Saved summary: {summary_json}\")\n",
        "    print(f\"\u2705 Saved threshold sweep: {sweep_csv}\")\n",
        "    print(f\"\u2705 Saved predictions: {preds_csv}\")\n",
        "    print(f\"\u2705 Saved daily metrics: {daily_csv_w}\")\n",
        "\n",
        "    # Visualization 1: Daily accuracy in window\n",
        "    if len(daily_w):\n",
        "        fig, ax = plt.subplots(figsize=(12, 4.5), dpi=140)\n",
        "        dplot = daily_w.copy()\n",
        "        dplot['acc_ma'] = dplot['acc'].rolling(window=7, min_periods=1).mean()\n",
        "        sizes = 20 + 3.0 * dplot['n'].astype(float)\n",
        "        sc = ax.scatter(\n",
        "            dplot['date_dt'], dplot['acc'],\n",
        "            c=dplot['n'], cmap='viridis', s=sizes,\n",
        "            alpha=0.9, edgecolor='k', linewidth=0.3, label='Daily accuracy'\n",
        "        )\n",
        "        ax.plot(dplot['date_dt'], dplot['acc'], color='gray', alpha=0.35, linewidth=1)\n",
        "        ax.plot(dplot['date_dt'], dplot['acc_ma'], color='#d62728', linewidth=2.2, label='7-day moving avg')\n",
        "        ax.axhline(acc_w, color='#1f77b4', linestyle='--', linewidth=1.5, label=f'Window overall: {acc_w:.2%}')\n",
        "        ax.set_title(f'Daily Accuracy ({start_date.date()} \u2192 {end_date.date()})', fontsize=13)\n",
        "        ax.set_xlabel('Date'); ax.set_ylabel('Accuracy'); ax.set_ylim(0, 1)\n",
        "        ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
        "        locator = mdates.AutoDateLocator(minticks=6, maxticks=10)\n",
        "        ax.xaxis.set_major_locator(locator)\n",
        "        ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator))\n",
        "        ax.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax.legend(loc='lower left')\n",
        "        cbar = plt.colorbar(sc, ax=ax, pad=0.015); cbar.set_label('Samples per day (n)')\n",
        "        plt.tight_layout()\n",
        "        out_plot1 = os.path.join(OUTPUT_DIR, f'period_daily_accuracy_{period_tag}.png')\n",
        "        plt.savefig(out_plot1, bbox_inches='tight'); print(f\"\ud83d\uddbc\ufe0f Saved plot: {out_plot1}\")\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Visualization 2: Probability separation + reliability\n",
        "    if len(np.unique(y_true)) == 2 and len(y_true) >= 6:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), dpi=140)\n",
        "        # Prob distributions\n",
        "        axes[0].hist(p_up[y_true == 1], bins=15, alpha=0.7, label='True Up', color='#1f77b4')\n",
        "        axes[0].hist(p_up[y_true == 0], bins=15, alpha=0.7, label='True Down', color='#ff7f0e')\n",
        "        axes[0].axvline(THRESHOLD, color='k', linestyle='--', linewidth=1.2, label=f'Threshold {THRESHOLD:.2f}')\n",
        "        axes[0].set_title('Probabilities by Class')\n",
        "        axes[0].set_xlabel('P(up)'); axes[0].set_ylabel('Count'); axes[0].legend()\n",
        "        axes[0].grid(True, linestyle='--', alpha=0.3)\n",
        "        # Reliability curve (adaptive bins for small samples)\n",
        "        n_bins = max(3, min(10, len(y_true) // 2))\n",
        "        prob_true, prob_pred = calibration_curve(y_true, p_up, n_bins=n_bins, strategy='quantile')\n",
        "        axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfectly calibrated')\n",
        "        axes[1].plot(prob_pred, prob_true, marker='o', color='#2ca02c', linewidth=1.5, label='Model')\n",
        "        axes[1].set_title(f'Reliability (Brier={np.mean((p_up - y_true) ** 2):.4f})')\n",
        "        axes[1].set_xlabel('Predicted P(up)'); axes[1].set_ylabel('Observed frequency')\n",
        "        axes[1].set_xlim(0, 1); axes[1].set_ylim(0, 1); axes[1].grid(True, linestyle='--', alpha=0.3)\n",
        "        axes[1].legend()\n",
        "        plt.tight_layout()\n",
        "        out_plot2 = os.path.join(OUTPUT_DIR, f'period_distribution_reliability_{period_tag}.png')\n",
        "        plt.savefig(out_plot2, bbox_inches='tight'); print(f\"\ud83d\uddbc\ufe0f Saved plot: {out_plot2}\")\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Threshold sweep highlights\n",
        "    print(\"\\n\ud83d\udd27 Threshold sensitivity (top by accuracy):\")\n",
        "    print(best_by_acc)\n",
        "    print(\"\\n\ud83d\udd27 Threshold sensitivity (top by F1):\")\n",
        "    print(best_by_f1)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}