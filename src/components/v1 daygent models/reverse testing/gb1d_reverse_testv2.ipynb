{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "02EveUOqjWav",
        "outputId": "34e623fa-41d1-4197-f906-0361b188cf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Setting up dependencies...\n",
            "‚úÖ Core dependencies already available\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted (Colab environment)\n",
            "üîÑ GB1D Model Reverse Test - Loading and Verifying Saved Model\n",
            "Target: Reproduce EXACT same results as original gb1d_iso.ipynb run\n",
            "Expected test accuracy: 0.7143, test AUC: 0.7733, threshold: 0.57\n",
            "================================================================================\n",
            "‚úÖ Original model directory: /content/drive/MyDrive/daygent_v1_models/gb_1d_versionlock\n",
            "‚úÖ Reverse test output directory: /content/drive/MyDrive/daygent_v1_models/gb_1d_reverse_versionlock\n",
            "‚úÖ Data directory: /content/drive/MyDrive/daygent_v1_models/spy_data_export\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 1: CROSS-PLATFORM DEPENDENCY MANAGEMENT\n",
        "# ========================================\n",
        "print(\"üîß Setting up dependencies...\")\n",
        "\n",
        "# Cross-platform dependency installation\n",
        "try:\n",
        "    import pandas, numpy, sklearn, xgboost, matplotlib, seaborn, joblib, tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import joblib\n",
        "    import lightgbm as lgb  # not used here, keeps env parity\n",
        "    print(\"‚úÖ Core dependencies already available\")\n",
        "except ImportError as e:\n",
        "    print(f\"Installing missing dependencies: {e}\")\n",
        "    import sys, subprocess\n",
        "    pkgs = ['pandas', 'numpy', 'scikit-learn', 'xgboost', 'lightgbm',\n",
        "            'matplotlib', 'seaborn', 'joblib', 'tqdm', 'pyarrow']\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + pkgs)\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import joblib\n",
        "    print(\"‚úÖ Dependencies installed\")\n",
        "\n",
        "# Try to mount Google Drive if available (Colab environment)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    BASE_DIR = '/content/drive/MyDrive/daygent_v1_models'  # same base folder as your LGBM 4h\n",
        "    print(\"‚úÖ Google Drive mounted (Colab environment)\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    BASE_DIR = './daygent_v1_models'\n",
        "    print(\"‚úÖ Local environment detected\")\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'spy_data_export')\n",
        "ORIGINAL_MODEL_DIR = os.path.join(BASE_DIR, 'gb_1d_versionlock')  # Load from original location\n",
        "REVERSE_MODEL_DIR = os.path.join(BASE_DIR, 'gb_1d_reverse_versionlock')  # Save reverse test results here\n",
        "os.makedirs(REVERSE_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üîÑ GB1D Model Reverse Test - Loading and Verifying Saved Model\")\n",
        "print(\"Target: Reproduce EXACT same results as original gb1d_iso.ipynb run\")\n",
        "print(\"Expected test accuracy: 0.7143, test AUC: 0.7733, threshold: 0.57\")\n",
        "print(\"=\"*80)\n",
        "print(f\"‚úÖ Original model directory: {ORIGINAL_MODEL_DIR}\")\n",
        "print(f\"‚úÖ Reverse test output directory: {REVERSE_MODEL_DIR}\")\n",
        "print(f\"‚úÖ Data directory: {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# VERSION EXTRACTION FOR ENVIRONMENT LOCKING\n",
        "# ========================================\n",
        "print(\"\\nüîí Extracting exact library versions for environment locking...\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "\n",
        "versions = {\n",
        "    'python': sys.version,\n",
        "    'pandas': pd.__version__,\n",
        "    'numpy': np.__version__,\n",
        "    'scikit-learn': sklearn.__version__,\n",
        "    'xgboost': xgb.__version__,\n",
        "    'lightgbm': lgb.__version__,\n",
        "    'joblib': joblib.__version__\n",
        "}\n",
        "\n",
        "print(\"üì¶ Current library versions:\")\n",
        "for lib, ver in versions.items():\n",
        "    print(f\"  {lib}: {ver}\")\n",
        "\n",
        "# Save versions to file for environment creation\n",
        "versions_file = os.path.join(REVERSE_MODEL_DIR, 'training_versions.json')\n",
        "with open(versions_file, 'w') as f:\n",
        "    json.dump(versions, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Versions saved to: {versions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gOP0zPZ1v39C",
        "outputId": "31b1aa1c-0551-4b28-c1dc-5a4facef580a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîí Extracting exact library versions for environment locking...\n",
            "üì¶ Current library versions:\n",
            "  python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "  pandas: 2.2.2\n",
            "  numpy: 2.0.2\n",
            "  scikit-learn: 1.6.1\n",
            "  xgboost: 3.0.4\n",
            "  lightgbm: 4.6.0\n",
            "  joblib: 1.5.1\n",
            "‚úÖ Versions saved to: /content/drive/MyDrive/daygent_v1_models/gb_1d_reverse_versionlock/training_versions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K-ki0UktjWa0",
        "outputId": "01132d42-5c17-4af0-9783-5ebf9267ffe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Loading saved model artifacts...\n",
            "‚úÖ Model loaded: GradientBoostingClassifier\n",
            "‚úÖ Scaler loaded: StandardScaler\n",
            "üìä Feature count: 16\n",
            "üéØ Calibrated threshold: 0.57\n",
            "üìÖ Original test period: 2024-12-17 to 2025-02-07\n",
            "üìã Original predictions loaded: 35 samples\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 2: LOAD SAVED MODEL ARTIFACTS\n",
        "# ========================================\n",
        "print(\"\\nüîß Loading saved model artifacts...\")\n",
        "\n",
        "# Load configuration files\n",
        "with open(os.path.join(ORIGINAL_MODEL_DIR, 'results_gb_1d.json'), 'r') as f:\n",
        "    original_results = json.load(f)\n",
        "\n",
        "with open(os.path.join(ORIGINAL_MODEL_DIR, 'deployment_config.json'), 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Load model and scaler\n",
        "model_path = os.path.join(ORIGINAL_MODEL_DIR, 'gb_1d_final.joblib')\n",
        "scaler_path = os.path.join(ORIGINAL_MODEL_DIR, 'scaler_1d.joblib')\n",
        "\n",
        "gb_model = joblib.load(model_path)\n",
        "scaler = joblib.load(scaler_path)\n",
        "\n",
        "# Extract key parameters\n",
        "FEATURE_NAMES = config['feature_names']\n",
        "THRESHOLD = config['calibrated_threshold']\n",
        "TEST_PERIOD = original_results['test_period']\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {type(gb_model).__name__}\")\n",
        "print(f\"‚úÖ Scaler loaded: {type(scaler).__name__}\")\n",
        "print(f\"üìä Feature count: {len(FEATURE_NAMES)}\")\n",
        "print(f\"üéØ Calibrated threshold: {THRESHOLD}\")\n",
        "print(f\"üìÖ Original test period: {TEST_PERIOD}\")\n",
        "\n",
        "# Load original predictions for comparison\n",
        "original_preds = pd.read_csv(os.path.join(ORIGINAL_MODEL_DIR, 'test_predictions_1d.csv'))\n",
        "print(f\"üìã Original predictions loaded: {len(original_preds)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F0oMnem6jWa1",
        "outputId": "5b3b9a81-2f45-41f9-a067-08a9b9e3b5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Loading raw data (matching original process)...\n",
            "‚úÖ Loaded 1d data: 2,547 candles\n",
            "üìÖ 1d range: 2014-12-23 14:30:00+00:00 to 2025-02-07 14:30:00+00:00\n",
            "‚úÖ Loaded 4h data: 3,058 candles\n",
            "üìÖ 4h range: 2019-01-07 14:30:00+00:00 to 2025-02-10 14:30:00+00:00\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 3: LOAD RAW DATA (EXACT SAME AS ORIGINAL)\n",
        "# ========================================\n",
        "print(\"\\nüìä Loading raw data (matching original process)...\")\n",
        "\n",
        "TIMEFRAMES_ORDERED = ['1d', '4h']\n",
        "raw_data = {}\n",
        "\n",
        "for tf in TIMEFRAMES_ORDERED:\n",
        "    csv_file = os.path.join(DATA_DIR, f'spy_{tf}.csv')\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise FileNotFoundError(f\"‚ùå {csv_file} not found!\")\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    raw_data[tf] = df\n",
        "    print(f\"‚úÖ Loaded {tf} data: {len(df):,} candles\")\n",
        "    print(f\"üìÖ {tf} range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F3l3iTzkjWa1",
        "outputId": "db06644d-a118-4654-d6a9-2d306bcc5484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Recreating exact test period determination...\n",
            "üìÖ Recreated test period: 2024-12-17 ‚Üí 2025-02-07 (35 trading days)\n",
            "üîç Original test period: 2024-12-17 to 2025-02-07\n",
            "‚úÖ Test period matches original exactly!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 4: RECREATE EXACT TEST PERIOD (MATCHING ORIGINAL LOGIC)\n",
        "# ========================================\n",
        "print(\"\\nüéØ Recreating exact test period determination...\")\n",
        "\n",
        "# Find common date range between 1d and 4h data\n",
        "latest_start = max(raw_data['1d']['timestamp'].min(), raw_data['4h']['timestamp'].min())\n",
        "earliest_end  = min(raw_data['1d']['timestamp'].max(), raw_data['4h']['timestamp'].max())\n",
        "\n",
        "# Find common trading days\n",
        "common_dates = set(raw_data['1d'][(raw_data['1d']['timestamp'] >= latest_start) &\n",
        "                                  (raw_data['1d']['timestamp'] <= earliest_end)]['timestamp'].dt.date.unique())\n",
        "common_dates &= set(raw_data['4h'][(raw_data['4h']['timestamp'] >= latest_start) &\n",
        "                                   (raw_data['4h']['timestamp'] <= earliest_end)]['timestamp'].dt.date.unique())\n",
        "\n",
        "# Select last 35 days (same as original)\n",
        "all_days = sorted(common_dates)\n",
        "TEST_DAYS = min(35, len(all_days))\n",
        "selected_days = all_days[-TEST_DAYS:]\n",
        "\n",
        "test_start = pd.Timestamp.combine(selected_days[0],  pd.Timestamp.min.time()).tz_localize('UTC')\n",
        "test_end   = pd.Timestamp.combine(selected_days[-1], pd.Timestamp.max.time()).tz_localize('UTC')\n",
        "\n",
        "print(f\"üìÖ Recreated test period: {test_start.date()} ‚Üí {test_end.date()} ({TEST_DAYS} trading days)\")\n",
        "print(f\"üîç Original test period: {TEST_PERIOD}\")\n",
        "\n",
        "# Verify we got the exact same period\n",
        "expected_start = \"2024-12-17\"\n",
        "expected_end = \"2025-02-07\"\n",
        "if str(test_start.date()) == expected_start and str(test_end.date()) == expected_end:\n",
        "    print(\"‚úÖ Test period matches original exactly!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Test period mismatch! Expected: {expected_start} to {expected_end}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2ZA5JFLFjWa2",
        "outputId": "d08d2c14-25d6-4ee8-e494-d020ff96b43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Setting up feature extraction (exact same as original)...\n",
            "‚úÖ Feature extraction functions ready\n",
            "üìã Expected feature names: ['raw_o', 'raw_h', 'raw_l', 'raw_c', 'raw_v', 'iso_0', 'iso_1', 'iso_2', 'iso_3', 'tf_1d', 'tf_4h', 'hl_range', 'price_change', 'upper_shadow', 'lower_shadow', 'volume_m']\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 5: FEATURE EXTRACTION FUNCTIONS (EXACT SAME AS ORIGINAL)\n",
        "# ========================================\n",
        "print(\"\\nüîß Setting up feature extraction (exact same as original)...\")\n",
        "\n",
        "def parse_vector_column(vector_str):\n",
        "    \"\"\"Parse vector string to numpy array\"\"\"\n",
        "    if pd.isna(vector_str) or vector_str is None:\n",
        "        return None\n",
        "    if isinstance(vector_str, str):\n",
        "        s = vector_str.strip('[]\"')\n",
        "        try:\n",
        "            return np.array([float(x.strip()) for x in s.split(',')])\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return np.array(vector_str)\n",
        "\n",
        "def build_feature_vector(raw_ohlcv, iso_ohlc, tf, tf_list):\n",
        "    \"\"\"Build 16-feature vector\"\"\"\n",
        "    o, h, l, c, v = raw_ohlcv\n",
        "    features = list(raw_ohlcv)          # 5\n",
        "    features.extend(list(iso_ohlc))     # 4\n",
        "    features.extend([1 if tf == t else 0 for t in tf_list])  # 2\n",
        "    features.extend([\n",
        "        (h - l) / c if c else 0,        # hl_range\n",
        "        (c - o) / o if o else 0,        # price_change\n",
        "        (h - c) / c if c else 0,        # upper_shadow\n",
        "        (c - l) / c if c else 0,        # lower_shadow\n",
        "        v / 1_000_000,                  # volume_m\n",
        "    ])  # 5\n",
        "    return np.array(features, dtype=float)\n",
        "\n",
        "def extract_features_1d(row):\n",
        "    raw_ohlcv = parse_vector_column(row.get('raw_ohlcv_vec'))\n",
        "    iso_ohlc  = parse_vector_column(row.get('iso_ohlc'))\n",
        "    future    = row.get('future')\n",
        "    if raw_ohlcv is None or iso_ohlc is None or pd.isna(future):\n",
        "        return None, None\n",
        "    if len(raw_ohlcv) != 5 or len(iso_ohlc) != 4:\n",
        "        return None, None\n",
        "    return build_feature_vector(raw_ohlcv, iso_ohlc, '1d', TIMEFRAMES_ORDERED), int(future)\n",
        "\n",
        "print(f\"‚úÖ Feature extraction functions ready\")\n",
        "print(f\"üìã Expected feature names: {FEATURE_NAMES}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pUt0WGH0jWa2",
        "outputId": "04c22c08-8c67-4d1f-90e1-585b2ac56e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Extracting test features (matching original process)...\n",
            "üìä Test samples from data: 35\n",
            "üìä Expected test samples: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting test features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:00<00:00, 8139.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Extracted test features: (35, 16)\n",
            "üìä Test labels: 35\n",
            "‚úÖ Test sample count matches original exactly!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 6: EXTRACT TEST FEATURES (EXACT SAME LOGIC)\n",
        "# ========================================\n",
        "print(\"\\nüîÑ Extracting test features (matching original process)...\")\n",
        "\n",
        "df_1d = raw_data['1d']\n",
        "test_df = df_1d[(df_1d['timestamp'] >= test_start) & (df_1d['timestamp'] <= test_end)].copy()\n",
        "\n",
        "print(f\"üìä Test samples from data: {len(test_df)}\")\n",
        "print(f\"üìä Expected test samples: {original_results['test_samples']}\")\n",
        "\n",
        "# Extract test features and store detailed info\n",
        "X_test, y_test, test_timestamps = [], [], []\n",
        "test_rows_info = []\n",
        "\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Extracting test features\"):\n",
        "    fv, lbl = extract_features_1d(row)\n",
        "    if fv is not None:\n",
        "        X_test.append(fv)\n",
        "        y_test.append(lbl)\n",
        "        test_timestamps.append(row['timestamp'])\n",
        "        test_rows_info.append({\n",
        "            'timestamp': row['timestamp'],\n",
        "            'raw_ohlcv': parse_vector_column(row['raw_ohlcv_vec']),\n",
        "            'iso_ohlc':  parse_vector_column(row['iso_ohlc']),\n",
        "            'future': int(row['future']),\n",
        "            'feature_vector': fv\n",
        "        })\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"üìä Extracted test features: {X_test.shape}\")\n",
        "print(f\"üìä Test labels: {len(y_test)}\")\n",
        "\n",
        "# Verify we got exactly the same number of samples\n",
        "if len(X_test) == original_results['test_samples']:\n",
        "    print(\"‚úÖ Test sample count matches original exactly!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Sample count mismatch! Got {len(X_test)}, expected {original_results['test_samples']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z7wQ4RgrjWa3",
        "outputId": "de3f02fa-d8ab-4e86-9fd6-848713df814f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Making predictions with loaded model...\n",
            "\n",
            "üéØ REVERSE TEST RESULTS:\n",
            "‚úÖ Test Accuracy: 0.7143\n",
            "‚úÖ Test AUC: 0.7733\n",
            "üìä Threshold used: 0.57\n",
            "üìä Test predictions: [13 22]\n",
            "üìä Actual labels: [15 20]\n",
            "\n",
            "üîç COMPARISON WITH ORIGINAL:\n",
            "Original Test Accuracy: 0.7143\n",
            "Original Test AUC: 0.7733\n",
            "Original Threshold: 0.57\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 7: MAKE PREDICTIONS WITH LOADED MODEL\n",
        "# ========================================\n",
        "print(\"\\nüß† Making predictions with loaded model...\")\n",
        "\n",
        "# Scale test features with loaded scaler (same as original)\n",
        "X_test_scaled = scaler.transform(X_test) if len(X_test) else np.empty((0, len(FEATURE_NAMES)))\n",
        "\n",
        "# Make predictions\n",
        "test_pred_proba = gb_model.predict_proba(X_test_scaled)[:, 1] if len(X_test_scaled) else np.array([])\n",
        "test_pred = (test_pred_proba >= THRESHOLD).astype(int) if len(test_pred_proba) else np.array([])\n",
        "\n",
        "# Calculate metrics\n",
        "test_acc = accuracy_score(y_test, test_pred) if len(test_pred) else float('nan')\n",
        "test_auc = roc_auc_score(y_test, test_pred_proba) if (len(test_pred_proba) and len(np.unique(y_test))==2) else float('nan')\n",
        "\n",
        "print(f\"\\nüéØ REVERSE TEST RESULTS:\")\n",
        "print(f\"‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"‚úÖ Test AUC: {test_auc:.4f}\")\n",
        "print(f\"üìä Threshold used: {THRESHOLD}\")\n",
        "if len(test_pred):\n",
        "    print(f\"üìä Test predictions: {np.bincount(test_pred)}\")\n",
        "    print(f\"üìä Actual labels: {np.bincount(y_test)}\")\n",
        "\n",
        "print(f\"\\nüîç COMPARISON WITH ORIGINAL:\")\n",
        "print(f\"Original Test Accuracy: {original_results['test_accuracy']:.4f}\")\n",
        "print(f\"Original Test AUC: {original_results['test_auc']:.4f}\")\n",
        "print(f\"Original Threshold: {original_results['chosen_threshold']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j5yswRnYjWa3",
        "outputId": "a1c5a9b9-0064-49ff-b6fc-eb66d3dc50b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Detailed prediction-by-prediction comparison...\n",
            "üìä New predictions table: 35 rows\n",
            "üìä Original predictions table: 35 rows\n",
            "\n",
            "üîç Comparing key prediction columns:\n",
            "  pred_prob_up: ‚úÖ EXACT MATCH (max diff: 1.11e-16)\n",
            "  pred_label: ‚úÖ EXACT MATCH\n",
            "  true_label: ‚úÖ EXACT MATCH\n",
            "  correct: ‚úÖ EXACT MATCH\n",
            "\n",
            "üéØ OVERALL COMPARISON: ‚úÖ ALL PREDICTIONS MATCH EXACTLY!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 8: DETAILED PREDICTION-BY-PREDICTION COMPARISON\n",
        "# ========================================\n",
        "print(\"\\nüîç Detailed prediction-by-prediction comparison...\")\n",
        "\n",
        "# Build new predictions table\n",
        "new_records = []\n",
        "for i, info in enumerate(test_rows_info):\n",
        "    ts   = info['timestamp']\n",
        "    fv   = info['feature_vector']\n",
        "    raw  = info['raw_ohlcv']\n",
        "    iso  = info['iso_ohlc']\n",
        "    true = info['future']\n",
        "\n",
        "    proba = float(test_pred_proba[i])\n",
        "    pred  = int(test_pred[i])\n",
        "    correct = bool(pred == true)\n",
        "    margin = proba - THRESHOLD\n",
        "\n",
        "    rec = {\n",
        "        'candle_index_in_test': i + 1,\n",
        "        'timestamp_utc': ts,\n",
        "        'date_utc': ts.date(),\n",
        "        'pred_prob_up': proba,\n",
        "        'pred_label': int(pred),\n",
        "        'true_label': int(true),\n",
        "        'correct': correct,\n",
        "        'threshold_used': THRESHOLD,\n",
        "        'decision_margin': margin,\n",
        "        'raw_o': raw[0], 'raw_h': raw[1], 'raw_l': raw[2], 'raw_c': raw[3], 'raw_v': raw[4],\n",
        "        'iso_0': iso[0], 'iso_1': iso[1], 'iso_2': iso[2], 'iso_3': iso[3],\n",
        "        'tf_1d': fv[FEATURE_NAMES.index('tf_1d')],\n",
        "        'tf_4h': fv[FEATURE_NAMES.index('tf_4h')],\n",
        "        'hl_range': fv[FEATURE_NAMES.index('hl_range')],\n",
        "        'price_change': fv[FEATURE_NAMES.index('price_change')],\n",
        "        'upper_shadow': fv[FEATURE_NAMES.index('upper_shadow')],\n",
        "        'lower_shadow': fv[FEATURE_NAMES.index('lower_shadow')],\n",
        "        'volume_m': fv[FEATURE_NAMES.index('volume_m')],\n",
        "    }\n",
        "    new_records.append(rec)\n",
        "\n",
        "new_pred_df = pd.DataFrame.from_records(new_records).sort_values(['date_utc','timestamp_utc']).reset_index(drop=True)\n",
        "\n",
        "print(f\"üìä New predictions table: {len(new_pred_df)} rows\")\n",
        "print(f\"üìä Original predictions table: {len(original_preds)} rows\")\n",
        "\n",
        "# Compare key columns\n",
        "comparison_cols = ['pred_prob_up', 'pred_label', 'true_label', 'correct']\n",
        "print(\"\\nüîç Comparing key prediction columns:\")\n",
        "\n",
        "all_match = True\n",
        "for col in comparison_cols:\n",
        "    if col in original_preds.columns and col in new_pred_df.columns:\n",
        "        if col == 'pred_prob_up':\n",
        "            # For probabilities, allow small floating point differences\n",
        "            diff = np.abs(original_preds[col].values - new_pred_df[col].values)\n",
        "            max_diff = np.max(diff)\n",
        "            matches = np.allclose(original_preds[col].values, new_pred_df[col].values, rtol=1e-10, atol=1e-10)\n",
        "            print(f\"  {col}: {'‚úÖ EXACT MATCH' if matches else '‚ùå MISMATCH'} (max diff: {max_diff:.2e})\")\n",
        "            if not matches:\n",
        "                all_match = False\n",
        "        else:\n",
        "            matches = (original_preds[col].values == new_pred_df[col].values).all()\n",
        "            print(f\"  {col}: {'‚úÖ EXACT MATCH' if matches else '‚ùå MISMATCH'}\")\n",
        "            if not matches:\n",
        "                all_match = False\n",
        "    else:\n",
        "        print(f\"  {col}: ‚ö†Ô∏è  Column not found in one of the datasets\")\n",
        "        all_match = False\n",
        "\n",
        "print(f\"\\nüéØ OVERALL COMPARISON: {'‚úÖ ALL PREDICTIONS MATCH EXACTLY!' if all_match else '‚ùå SOME DIFFERENCES FOUND'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dd4TNQ8YjWa4",
        "outputId": "61478260-4365-4288-bfbd-9749b231a8b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL VALIDATION SUMMARY\n",
            "================================================================================\n",
            "üìä Test Accuracy Match: ‚úÖ (0.714286 vs 0.714286)\n",
            "üìä Test AUC Match: ‚úÖ (0.773333 vs 0.773333)\n",
            "üìä Threshold Match: ‚úÖ (0.57 vs 0.57)\n",
            "üìä Sample Count Match: ‚úÖ (35 vs 35)\n",
            "üìä Prediction Details Match: ‚úÖ\n",
            "\n",
            "üéØ REVERSE TEST RESULT: üéâ PERFECT MATCH! Model loaded and reproduced identical results.\n",
            "\n",
            "‚úÖ The saved GB1D model has been successfully validated!\n",
            "‚úÖ All predictions, metrics, and results match the original training run exactly.\n",
            "‚úÖ The model can be confidently deployed for production use.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 9: FINAL VALIDATION SUMMARY\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL VALIDATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Metrics comparison\n",
        "acc_match = abs(test_acc - original_results['test_accuracy']) < 1e-10\n",
        "auc_match = abs(test_auc - original_results['test_auc']) < 1e-10\n",
        "threshold_match = abs(THRESHOLD - original_results['chosen_threshold']) < 1e-10\n",
        "\n",
        "print(f\"üìä Test Accuracy Match: {'‚úÖ' if acc_match else '‚ùå'} ({test_acc:.6f} vs {original_results['test_accuracy']:.6f})\")\n",
        "print(f\"üìä Test AUC Match: {'‚úÖ' if auc_match else '‚ùå'} ({test_auc:.6f} vs {original_results['test_auc']:.6f})\")\n",
        "print(f\"üìä Threshold Match: {'‚úÖ' if threshold_match else '‚ùå'} ({THRESHOLD} vs {original_results['chosen_threshold']})\")\n",
        "print(f\"üìä Sample Count Match: {'‚úÖ' if len(X_test) == original_results['test_samples'] else '‚ùå'} ({len(X_test)} vs {original_results['test_samples']})\")\n",
        "print(f\"üìä Prediction Details Match: {'‚úÖ' if all_match else '‚ùå'}\")\n",
        "\n",
        "# Overall validation\n",
        "perfect_match = acc_match and auc_match and threshold_match and len(X_test) == original_results['test_samples'] and all_match\n",
        "\n",
        "print(f\"\\nüéØ REVERSE TEST RESULT: {'üéâ PERFECT MATCH! Model loaded and reproduced identical results.' if perfect_match else '‚ö†Ô∏è  Some differences detected. Review above for details.'}\")\n",
        "\n",
        "if perfect_match:\n",
        "    print(\"\\n‚úÖ The saved GB1D model has been successfully validated!\")\n",
        "    print(\"‚úÖ All predictions, metrics, and results match the original training run exactly.\")\n",
        "    print(\"‚úÖ The model can be confidently deployed for production use.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Validation failed. The loaded model does not reproduce identical results.\")\n",
        "    print(\"‚ö†Ô∏è  This could indicate:\")\n",
        "    print(\"   - Data loading differences\")\n",
        "    print(\"   - Feature extraction differences\")\n",
        "    print(\"   - Model/scaler loading issues\")\n",
        "    print(\"   - Random seed or environment differences\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J69g2lmEjWa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4fc0ab21-92a3-40a1-a82d-3a92a76ae96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saving reverse test results...\n",
            "‚úÖ Reverse test results saved to: /content/drive/MyDrive/daygent_v1_models/gb_1d_reverse_versionlock/gb1d_reverse_test_results.json\n",
            "‚úÖ New predictions saved to: /content/drive/MyDrive/daygent_v1_models/gb_1d_reverse_versionlock/gb1d_reverse_test_predictions.csv\n",
            "\n",
            "üèÅ Reverse test complete!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 10: SAVE REVERSE TEST RESULTS\n",
        "# ========================================\n",
        "print(\"\\nüíæ Saving reverse test results...\")\n",
        "\n",
        "reverse_test_results = {\n",
        "    'reverse_test_date': datetime.now().isoformat(),\n",
        "    'perfect_match': perfect_match,\n",
        "    'loaded_model_results': {\n",
        "        'test_accuracy': float(test_acc),\n",
        "        'test_auc': float(test_auc),\n",
        "        'threshold_used': float(THRESHOLD),\n",
        "        'test_samples': int(len(X_test)),\n",
        "        'predictions_match': all_match\n",
        "    },\n",
        "    'original_results': original_results,\n",
        "    'differences': {\n",
        "        'accuracy_diff': float(abs(test_acc - original_results['test_accuracy'])),\n",
        "        'auc_diff': float(abs(test_auc - original_results['test_auc'])),\n",
        "        'sample_count_match': len(X_test) == original_results['test_samples']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results to the reverse test directory\n",
        "results_path = os.path.join(REVERSE_MODEL_DIR, 'gb1d_reverse_test_results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(reverse_test_results, f, indent=2)\n",
        "\n",
        "# Save the new predictions for manual inspection if needed\n",
        "preds_path = os.path.join(REVERSE_MODEL_DIR, 'gb1d_reverse_test_predictions.csv')\n",
        "new_pred_df.to_csv(preds_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Reverse test results saved to: {results_path}\")\n",
        "print(f\"‚úÖ New predictions saved to: {preds_path}\")\n",
        "print(\"\\nüèÅ Reverse test complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}