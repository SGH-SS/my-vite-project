{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "LightGBM 1D Training Script (ISO-style features)\n",
        "\n",
        "This script mirrors the structure and behavior of `lgbm_iso.ipynb` but targets\n",
        "the SPY 1D timeframe using the combined full-data CSVs shown in your Drive:\n",
        "  MyDrive/daygent_v1_models/combined_spy_data/combined_spy_1d.csv\n",
        "\n",
        "Key behavior:\n",
        " - Cross-platform dependency setup (optional install) and optional Google Drive mount\n",
        " - Data source: combined_spy_data/combined_spy_1d.csv\n",
        " - Restrict to the last 6 years of data\n",
        " - Evaluation window: last 65 trading days (1D candles)\n",
        " - Feature contract: same 16 features as used in the ISO notebooks\n",
        " - Scaler fit on first 80% of pre-test training data\n",
        " - LightGBM params identical to 4H notebook\n",
        " - Saves model, scaler, results JSON, predictions CSV, detailed TXT report,\n",
        "   and deployment artifacts under: daygent_v1_models/lgbm_1d\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import List, Optional, Tuple\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 1: SETUP DEPENDENCIES & ENV\n",
        "print(\"\ud83d\udd27 Setting up dependencies...\")\n",
        "\n",
        "# Cross-platform dependency installation (best-effort)\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    import joblib\n",
        "    import lightgbm as lgb\n",
        "    print(\"\u2705 Core dependencies already available\")\n",
        "except Exception as e:  # pragma: no cover\n",
        "    print(f\"Installing missing dependencies: {e}\")\n",
        "    import sys, subprocess\n",
        "    pkgs = [\n",
        "        \"pandas\",\n",
        "        \"numpy\",\n",
        "        \"scikit-learn\",\n",
        "        \"lightgbm\",\n",
        "        \"matplotlib\",\n",
        "        \"seaborn\",\n",
        "        \"joblib\",\n",
        "        \"tqdm\",\n",
        "        \"pyarrow\",\n",
        "    ]\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs)\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    import joblib\n",
        "    import lightgbm as lgb\n",
        "    print(\"\u2705 Dependencies installed\")\n",
        "\n",
        "# Optional Google Drive mount (Colab)\n",
        "try:  # pragma: no cover\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\")\n",
        "    IS_COLAB = True\n",
        "    BASE_DIR = \"/content/drive/MyDrive/daygent_v1_models\"\n",
        "    print(\"\u2705 Google Drive mounted (Colab environment)\")\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "    BASE_DIR = \"./daygent_v1_models\"\n",
        "    print(\"\u2705 Local environment detected\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 2: DIRECTORIES\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Directories\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"combined_spy_data\")\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"lgbm_1d_iso\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\u2705 Model directory: {MODEL_DIR}\")\n",
        "print(f\"\u2705 Data directory: {DATA_DIR}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 3: HELPERS & FEATURE CONTRACT\n",
        "\n",
        "def parse_vector_column(vector_value) -> Optional[np.ndarray]:\n",
        "    \"\"\"Parse a vector column (stringified list or list-like) into np.array.\n",
        "\n",
        "    Accepts values like:\n",
        "      \"[1.0, 2.0, 3.0]\"\n",
        "      \"1.0, 2.0, 3.0\"\n",
        "      list/array-like\n",
        "    Returns None if parsing fails.\n",
        "    \"\"\"\n",
        "    if vector_value is None or (isinstance(vector_value, float) and np.isnan(vector_value)):\n",
        "        return None\n",
        "    if isinstance(vector_value, str):\n",
        "        s = vector_value.strip().strip(\"[]\\\"\")\n",
        "        if not s:\n",
        "            return None\n",
        "        try:\n",
        "            return np.array([float(x.strip()) for x in s.split(\",\")])\n",
        "        except Exception:\n",
        "            return None\n",
        "    try:\n",
        "        return np.array(vector_value, dtype=float)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "TIMEFRAMES_ORDERED: List[str] = [\"1d\", \"4h\"]\n",
        "\n",
        "FEATURE_NAMES: List[str] = [\n",
        "    \"raw_o\",\n",
        "    \"raw_h\",\n",
        "    \"raw_l\",\n",
        "    \"raw_c\",\n",
        "    \"raw_v\",\n",
        "    \"iso_0\",\n",
        "    \"iso_1\",\n",
        "    \"iso_2\",\n",
        "    \"iso_3\",\n",
        "    \"tf_1d\",\n",
        "    \"tf_4h\",\n",
        "    \"hl_range\",\n",
        "    \"price_change\",\n",
        "    \"upper_shadow\",\n",
        "    \"lower_shadow\",\n",
        "    \"volume_m\",\n",
        "]\n",
        "\n",
        "\n",
        "def build_feature_vector_1d(raw_ohlcv: np.ndarray, iso_ohlc: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Build the 16-feature vector for a 1D sample.\n",
        "\n",
        "    Order matches FEATURE_NAMES above and the ISO notebooks' contract.\n",
        "    \"\"\"\n",
        "    if len(raw_ohlcv) != 5 or len(iso_ohlc) != 4:\n",
        "        raise ValueError(\"Bad vector lengths for raw_ohlcv or iso_ohlc\")\n",
        "\n",
        "    o, h, l, c, v = raw_ohlcv\n",
        "    features: List[float] = []\n",
        "\n",
        "    # Raw OHLCV (5)\n",
        "    features.extend([o, h, l, c, v])\n",
        "\n",
        "    # ISO (4)\n",
        "    features.extend(list(iso_ohlc))\n",
        "\n",
        "    # One-hot timeframe for ['1d', '4h'] -> 1d=[1,0]\n",
        "    features.extend([1, 0])\n",
        "\n",
        "    # Engineered (5)\n",
        "    hl_range = (h - l) / c if c else 0.0\n",
        "    price_change = (c - o) / o if o else 0.0\n",
        "    upper_shadow = (h - c) / c if c else 0.0\n",
        "    lower_shadow = (c - l) / c if c else 0.0\n",
        "    volume_m = v / 1_000_000.0\n",
        "    features.extend([hl_range, price_change, upper_shadow, lower_shadow, volume_m])\n",
        "\n",
        "    return np.array(features, dtype=float)\n",
        "\n",
        "\n",
        "def extract_features_1d_only(row: pd.Series) -> Tuple[Optional[np.ndarray], Optional[int]]:\n",
        "    raw_ohlcv = parse_vector_column(row.get(\"raw_ohlcv_vec\"))\n",
        "    iso_ohlc = parse_vector_column(row.get(\"iso_ohlc\"))\n",
        "    future = row.get(\"future\")\n",
        "    if raw_ohlcv is None or iso_ohlc is None or future is None or (isinstance(future, float) and np.isnan(future)):\n",
        "        return None, None\n",
        "    try:\n",
        "        fv = build_feature_vector_1d(raw_ohlcv, iso_ohlc)\n",
        "        return fv, int(future)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 4: LOAD 1D DATA & SLICE TO LAST 6 YEARS\n",
        "\n",
        "print(\"\\n\ud83d\udcca Loading 1D full-data CSV from combined_spy_data...\")\n",
        "csv_file = os.path.join(DATA_DIR, \"combined_spy_1d.csv\")\n",
        "if not os.path.exists(csv_file):\n",
        "    raise FileNotFoundError(f\"\u274c {csv_file} not found. Ensure your Drive has combined_spy_data/combined_spy_1d.csv\")\n",
        "\n",
        "df_1d = pd.read_csv(csv_file)\n",
        "if \"timestamp\" not in df_1d.columns:\n",
        "    raise RuntimeError(\"\u274c Expected a 'timestamp' column in combined_spy_1d.csv\")\n",
        "\n",
        "df_1d[\"timestamp\"] = pd.to_datetime(df_1d[\"timestamp\"])\n",
        "df_1d = df_1d.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "print(f\"\u2705 Loaded 1d data: {len(df_1d):,} candles\")\n",
        "print(f\"\ud83d\udcc5 Range: {df_1d['timestamp'].min()} \u2192 {df_1d['timestamp'].max()}\")\n",
        "\n",
        "# Restrict to the last 6 years\n",
        "latest_ts = df_1d[\"timestamp\"].max()\n",
        "cutoff_ts = latest_ts - pd.DateOffset(years=6)\n",
        "df_1d = df_1d[df_1d[\"timestamp\"] >= cutoff_ts].copy()\n",
        "df_1d = df_1d.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "print(f\"\ud83d\uddc2\ufe0f Using last 6 years only: {df_1d['timestamp'].min()} \u2192 {df_1d['timestamp'].max()}  ({len(df_1d):,} rows)\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 5: DEFINE TEST PERIOD (LAST 65 TRADING DAYS)\n",
        "\n",
        "all_days = list(pd.to_datetime(df_1d[\"timestamp\"]).dt.date.unique())\n",
        "if len(all_days) < 65:\n",
        "    raise RuntimeError(\"\u274c Not enough 1D candles for a 65-day evaluation window after 6y filter\")\n",
        "\n",
        "selected_days = all_days[-65:]\n",
        "test_start = pd.Timestamp.combine(selected_days[0], pd.Timestamp.min.time()).tz_localize(\"UTC\")\n",
        "test_end = pd.Timestamp.combine(selected_days[-1], pd.Timestamp.max.time()).tz_localize(\"UTC\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Test period (65 trading days): {test_start.date()} \u2192 {test_end.date()}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 6: FEATURE EXTRACTION (1D ONLY)\n",
        "\n",
        "print(\"\\n\ud83d\udd04 Extracting features from 1d data...\")\n",
        "\n",
        "train_df = df_1d[df_1d[\"timestamp\"] < test_start].copy()\n",
        "test_df = df_1d[(df_1d[\"timestamp\"] >= test_start) & (df_1d[\"timestamp\"] <= test_end)].copy()\n",
        "\n",
        "print(f\"\ud83d\udcca Train samples (rows before test window): {len(train_df):,}\")\n",
        "print(f\"\ud83d\udcca Test samples (rows inside 65-day window): {len(test_df):,}\")\n",
        "\n",
        "# Training features\n",
        "X_train: List[np.ndarray] = []\n",
        "y_train: List[int] = []\n",
        "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Extracting 1d train features\"):\n",
        "    fv, lbl = extract_features_1d_only(row)\n",
        "    if fv is not None:\n",
        "        X_train.append(fv)\n",
        "        y_train.append(lbl)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "print(f\"\\n\u2705 Training features extracted: {X_train.shape}\")\n",
        "if len(y_train):\n",
        "    print(f\"\ud83d\udcca Class distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "# Test features + raw info for reporting\n",
        "X_test: List[np.ndarray] = []\n",
        "y_test: List[int] = []\n",
        "test_rows_info: List[dict] = []\n",
        "\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Extracting 1d test features\"):\n",
        "    fv, lbl = extract_features_1d_only(row)\n",
        "    if fv is not None:\n",
        "        X_test.append(fv)\n",
        "        y_test.append(lbl)\n",
        "        test_rows_info.append({\n",
        "            \"timestamp\": row[\"timestamp\"],\n",
        "            \"raw_ohlcv\": parse_vector_column(row.get(\"raw_ohlcv_vec\")),\n",
        "            \"iso_ohlc\": parse_vector_column(row.get(\"iso_ohlc\")),\n",
        "            \"future\": int(row.get(\"future\")),\n",
        "            \"feature_vector\": fv,\n",
        "        })\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "print(f\"\ud83d\udcca Test features extracted: {X_test.shape}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 7: SCALE AND SPLIT (MATCHING W2 LOGIC)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "split_idx = int(len(X_train) * 0.8)\n",
        "print(f\"\\n\ud83d\udd27 Fitting scaler on first {split_idx:,} training samples...\")\n",
        "if split_idx > 0:\n",
        "    scaler.fit(X_train[:split_idx])\n",
        "else:\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_tr = X_train_scaled[:split_idx]\n",
        "X_val = X_train_scaled[split_idx:]\n",
        "y_tr = y_train[:split_idx]\n",
        "y_val = y_train[split_idx:]\n",
        "\n",
        "print(f\"\ud83d\udcca Training set: {X_tr.shape}\")\n",
        "print(f\"\ud83d\udcca Validation set: {X_val.shape}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 8: TRAIN LIGHTGBM (EXACT PARAMS) + CALIBRATE THR\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Training LightGBM (1d) with exact params...\")\n",
        "\n",
        "lgb_params = {\n",
        "    # Match v0 script settings\n",
        "    \"objective\": \"binary\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"metric\": [\"auc\", \"binary_logloss\"],\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 63,\n",
        "    \"max_depth\": 6,\n",
        "    \"min_child_samples\": 40,\n",
        "    \"feature_fraction\": 0.85,\n",
        "    \"bagging_fraction\": 0.85,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"lambda_l1\": 0.1,\n",
        "    \"lambda_l2\": 0.1,\n",
        "    \"min_gain_to_split\": 0.0,\n",
        "    \"n_estimators\": 4000,\n",
        "    \"n_jobs\": -1,\n",
        "    \"verbose\": -1,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "if IS_COLAB:  # Optional GPU acceleration\n",
        "    lgb_params[\"device_type\"] = \"gpu\"\n",
        "    lgb_params[\"gpu_device_id\"] = 0\n",
        "    print(\"\u2705 GPU acceleration enabled\")\n",
        "\n",
        "model = lgb.LGBMClassifier(**lgb_params)\n",
        "print(\"\ud83d\udd04 Training LightGBM (no early stopping - full n_estimators)...\")\n",
        "if len(X_tr) and len(y_tr):\n",
        "    model.fit(X_tr, y_tr)\n",
        "else:\n",
        "    # Fall back to all train data if split produced empty slice\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Validation metrics and threshold calibration\n",
        "if len(X_val):\n",
        "    val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "    val_pred = (val_pred_proba >= 0.5).astype(int)\n",
        "    val_acc = accuracy_score(y_val, val_pred)\n",
        "    val_auc = roc_auc_score(y_val, val_pred_proba)\n",
        "else:\n",
        "    val_pred_proba = np.array([])\n",
        "    val_acc = float(\"nan\")\n",
        "    val_auc = float(\"nan\")\n",
        "\n",
        "print(f\"\\n\u2705 Validation Accuracy (t=0.50): {val_acc:.4f}\")\n",
        "print(f\"\u2705 Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "best_thr = 0.5\n",
        "best_val_acc = val_acc\n",
        "if len(val_pred_proba):\n",
        "    for thr in np.round(np.arange(0.30, 0.801, 0.01), 2):\n",
        "        preds_thr = (val_pred_proba >= thr).astype(int)\n",
        "        acc_thr = accuracy_score(y_val, preds_thr)\n",
        "        if acc_thr > best_val_acc:\n",
        "            best_val_acc = acc_thr\n",
        "            best_thr = float(thr)\n",
        "\n",
        "print(f\"\u2705 Calibrated decision threshold on validation: {best_thr:.2f} (Acc={best_val_acc:.4f})\")\n",
        "\n",
        "# Refit on all in-sample (train + val)\n",
        "X_full = X_train_scaled\n",
        "y_full = y_train\n",
        "model_full = lgb.LGBMClassifier(**lgb_params)\n",
        "model_full.fit(X_full, y_full)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 9: TEST + DETAILED PREDICTION-BY-PRED REPORT\n",
        "\n",
        "print(f\"\\n\ud83e\uddea Testing on isolated {len(selected_days)}-day period (1d)...\")\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test) if len(X_test) else np.empty((0, X_full.shape[1]))\n",
        "test_pred_proba = model_full.predict_proba(X_test_scaled)[:, 1] if len(X_test_scaled) else np.array([])\n",
        "test_pred = (test_pred_proba >= best_thr).astype(int) if len(test_pred_proba) else np.array([])\n",
        "\n",
        "if len(test_pred):\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    test_auc = roc_auc_score(y_test, test_pred_proba) if len(np.unique(y_test)) == 2 else float(\"nan\")\n",
        "else:\n",
        "    test_acc = float(\"nan\")\n",
        "    test_auc = float(\"nan\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf TEST RESULTS (1d):\")\n",
        "print(f\"\u2705 Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"\u2705 Test AUC: {test_auc:.4f}\")\n",
        "if len(test_pred):\n",
        "    print(f\"\ud83d\udcca Test predictions: {np.bincount(test_pred)}\")\n",
        "    print(f\"\ud83d\udcca Actual labels: {np.bincount(y_test)}\")\n",
        "\n",
        "# Build detailed per-prediction table\n",
        "records: List[dict] = []\n",
        "for i, info in enumerate(test_rows_info):\n",
        "    ts = info[\"timestamp\"]\n",
        "    fv = info[\"feature_vector\"]\n",
        "    raw = info[\"raw_ohlcv\"]\n",
        "    iso = info[\"iso_ohlc\"]\n",
        "    true = info[\"future\"]\n",
        "    proba = float(test_pred_proba[i]) if len(test_pred_proba) else float(\"nan\")\n",
        "    pred = int(test_pred[i]) if len(test_pred) else int(0)\n",
        "    correct = bool(pred == true) if len(test_pred) else False\n",
        "    margin = proba - best_thr if len(test_pred_proba) else float(\"nan\")\n",
        "\n",
        "    rec = {\n",
        "        \"candle_index_in_test\": i + 1,\n",
        "        \"timestamp_utc\": ts,\n",
        "        \"date_utc\": pd.Timestamp(ts).date(),\n",
        "        \"pred_prob_up\": proba,\n",
        "        \"pred_label\": int(pred),\n",
        "        \"true_label\": int(true),\n",
        "        \"correct\": correct,\n",
        "        \"threshold_used\": best_thr,\n",
        "        \"decision_margin\": margin,\n",
        "        # Raw OHLCV & ISO\n",
        "        \"raw_o\": raw[0] if raw is not None else np.nan,\n",
        "        \"raw_h\": raw[1] if raw is not None else np.nan,\n",
        "        \"raw_l\": raw[2] if raw is not None else np.nan,\n",
        "        \"raw_c\": raw[3] if raw is not None else np.nan,\n",
        "        \"raw_v\": raw[4] if raw is not None else np.nan,\n",
        "        \"iso_0\": iso[0] if iso is not None else np.nan,\n",
        "        \"iso_1\": iso[1] if iso is not None else np.nan,\n",
        "        \"iso_2\": iso[2] if iso is not None else np.nan,\n",
        "        \"iso_3\": iso[3] if iso is not None else np.nan,\n",
        "        # Engineered from feature vector indices\n",
        "        \"tf_1d\": fv[FEATURE_NAMES.index(\"tf_1d\")],\n",
        "        \"tf_4h\": fv[FEATURE_NAMES.index(\"tf_4h\")],\n",
        "        \"hl_range\": fv[FEATURE_NAMES.index(\"hl_range\")],\n",
        "        \"price_change\": fv[FEATURE_NAMES.index(\"price_change\")],\n",
        "        \"upper_shadow\": fv[FEATURE_NAMES.index(\"upper_shadow\")],\n",
        "        \"lower_shadow\": fv[FEATURE_NAMES.index(\"lower_shadow\")],\n",
        "        \"volume_m\": fv[FEATURE_NAMES.index(\"volume_m\")],\n",
        "    }\n",
        "    records.append(rec)\n",
        "\n",
        "pred_df = pd.DataFrame.from_records(records).sort_values([\"date_utc\", \"timestamp_utc\"]).reset_index(drop=True)\n",
        "\n",
        "# Save artifacts: predictions CSV and human-readable TXT\n",
        "pred_csv_path = os.path.join(MODEL_DIR, \"test_predictions_1d.csv\")\n",
        "pred_df.to_csv(pred_csv_path, index=False)\n",
        "\n",
        "txt_lines: List[str] = []\n",
        "txt_lines.append(\"=\" * 90)\n",
        "txt_lines.append(\"LIGHTGBM 1D \u2014 DETAILED DAY-BY-DAY / PREDICTION-BY-PREDICTION REPORT\")\n",
        "txt_lines.append(\"=\" * 90)\n",
        "txt_lines.append(f\"Test period: {test_start.date()} \u2192 {test_end.date()}\")\n",
        "txt_lines.append(f\"Total test candles: {len(pred_df)}\")\n",
        "txt_lines.append(f\"Calibrated threshold: {best_thr:.2f}\")\n",
        "txt_lines.append(f\"Overall Test Accuracy: {test_acc:.4f}\")\n",
        "txt_lines.append(f\"Overall Test AUC: {test_auc:.4f}\")\n",
        "txt_lines.append(\"\")\n",
        "\n",
        "for day in pred_df[\"date_utc\"].unique():\n",
        "    day_block = pred_df[pred_df[\"date_utc\"] == day]\n",
        "    correct_n = int(day_block[\"correct\"].sum())\n",
        "    total_n = len(day_block)\n",
        "    txt_lines.append(\"-\" * 90)\n",
        "    txt_lines.append(f\"{day}  \u2014  Day accuracy: {correct_n}/{total_n}  ({(correct_n/total_n) if total_n else 0:.3f})\")\n",
        "    txt_lines.append(\"-\" * 90)\n",
        "    for _, r in day_block.iterrows():\n",
        "        dir_word = \"UP\" if r[\"pred_label\"] == 1 else \"DOWN\"\n",
        "        truth_word = \"UP\" if r[\"true_label\"] == 1 else \"DOWN\"\n",
        "        right_wrong = \"\u2705 CORRECT\" if r[\"correct\"] else \"\u274c WRONG\"\n",
        "        txt_lines.append(\n",
        "            f\"[{int(r['candle_index_in_test']):02d}] {r['timestamp_utc']}  \"\n",
        "            f\"pred={dir_word}  p_up={r['pred_prob_up']:.4f}  thr={r['threshold_used']:.2f}  \"\n",
        "            f\"margin={r['decision_margin']:.4f}  truth={truth_word}  \u2192 {right_wrong}\"\n",
        "        )\n",
        "        txt_lines.append(\n",
        "            f\"    OHLCV: O={r['raw_o']:.4f}, H={r['raw_h']:.4f}, L={r['raw_l']:.4f}, C={r['raw_c']:.4f}, V={r['raw_v']:.0f} | \"\n",
        "            f\"ISO: [{r['iso_0']:.4f}, {r['iso_1']:.4f}, {r['iso_2']:.4f}, {r['iso_3']:.4f}] | \"\n",
        "            f\"feats: hl={r['hl_range']:.4f}, dC={r['price_change']:.4f}, upSh={r['upper_shadow']:.4f}, \"\n",
        "            f\"loSh={r['lower_shadow']:.4f}, vol_m={r['volume_m']:.4f}\"\n",
        "        )\n",
        "    txt_lines.append(\"\")\n",
        "\n",
        "report_path = os.path.join(MODEL_DIR, \"lgbm_1d_day_by_day.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(txt_lines))\n",
        "\n",
        "print(f\"\\n\ud83d\udcdd Saved detailed TXT report to: {report_path}\")\n",
        "print(f\"\ud83e\uddfe Saved machine-readable predictions to: {pred_csv_path}\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 10: SAVE MODEL, SCALER, AND RESULTS\n",
        "\n",
        "print(\"\\n\ud83d\udcbe Saving model and results...\")\n",
        "\n",
        "model_path = os.path.join(MODEL_DIR, \"lightgbm_financial_1d_only.joblib\")\n",
        "scaler_path = os.path.join(MODEL_DIR, \"scaler_1d_only.joblib\")\n",
        "joblib.dump(model_full, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "\n",
        "def _to_py(v):\n",
        "    try:\n",
        "        if isinstance(v, (np.integer, np.int64, np.int32)):\n",
        "            return int(v)\n",
        "        if isinstance(v, (np.floating,)):\n",
        "            return float(v)\n",
        "        return v\n",
        "    except Exception:\n",
        "        return v\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"test_accuracy\": float(test_acc),\n",
        "    \"test_auc\": float(test_auc),\n",
        "    \"validation_accuracy\": float(best_val_acc),\n",
        "    \"validation_auc\": float(val_auc) if not (isinstance(val_auc, float) and np.isnan(val_auc)) else None,\n",
        "    \"train_samples\": int(len(X_tr)) if len(X_tr) else int(len(X_train_scaled)),\n",
        "    \"val_samples\": int(len(X_val)),\n",
        "    \"test_samples\": int(len(X_test)),\n",
        "    \"feature_count\": int(X_full.shape[1]) if X_full.ndim == 2 else 0,\n",
        "    \"chosen_threshold\": float(best_thr),\n",
        "    \"model_params\": {k: _to_py(v) for k, v in lgb_params.items()},\n",
        "    \"feature_names\": FEATURE_NAMES,\n",
        "    \"report_txt\": os.path.basename(report_path),\n",
        "    \"predictions_csv\": os.path.basename(pred_csv_path),\n",
        "    \"model_path\": os.path.basename(model_path),\n",
        "    \"scaler_path\": os.path.basename(scaler_path),\n",
        "    \"test_period\": f\"{test_start.date()} to {test_end.date()}\",\n",
        "}\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"results_1d_only.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\u2705 Model saved to: {model_path}\")\n",
        "print(f\"\u2705 Scaler saved to: {scaler_path}\")\n",
        "print(\"\u2705 Results JSON saved as: results_1d_only.json\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 11: SAVE DEPLOYMENT ARTIFACTS (for your site)\n",
        "\n",
        "deployment_config = {\n",
        "    \"model_type\": \"LightGBMClassifier\",\n",
        "    \"timeframe\": \"1d\",\n",
        "    \"feature_contract_version\": \"v1\",\n",
        "    \"feature_names\": FEATURE_NAMES,\n",
        "    \"calibrated_threshold\": float(best_thr),\n",
        "    \"artifact_paths\": {\n",
        "        \"model_joblib\": os.path.basename(model_path),\n",
        "        \"scaler_joblib\": os.path.basename(scaler_path),\n",
        "    },\n",
        "    \"inference_notes\": {\n",
        "        \"scaling\": \"StandardScaler fitted on first 80% of pre-test training data\",\n",
        "        \"one_hot\": {\"tf_1d\": 1, \"tf_4h\": 0},\n",
        "        \"expected_columns_in_csv\": [\"timestamp\", \"raw_ohlcv_vec\", \"iso_ohlc\", \"future\"],\n",
        "    },\n",
        "    \"lgbm_params\": {k: _to_py(v) for k, v in lgb_params.items()},\n",
        "}\n",
        "\n",
        "config_path = os.path.join(MODEL_DIR, \"deployment_config.json\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(deployment_config, f, indent=2)\n",
        "\n",
        "feature_schema = {\n",
        "    \"raw_ohlcv_vec\": {\n",
        "        \"desc\": \"Stringified list of [open, high, low, close, volume]\",\n",
        "        \"len\": 5,\n",
        "        \"dtype\": \"float\",\n",
        "    },\n",
        "    \"iso_ohlc\": {\n",
        "        \"desc\": \"Stringified list of 4 ISO-normalized OHLC values\",\n",
        "        \"len\": 4,\n",
        "        \"dtype\": \"float\",\n",
        "    },\n",
        "    \"engineered\": [\n",
        "        \"hl_range=(H-L)/C\",\n",
        "        \"price_change=(C-O)/O\",\n",
        "        \"upper_shadow=(H-C)/C\",\n",
        "        \"lower_shadow=(C-L)/C\",\n",
        "        \"volume_m=V/1e6\",\n",
        "    ],\n",
        "    \"tf_one_hot\": {\"tf_1d\": 1, \"tf_4h\": 0},\n",
        "}\n",
        "\n",
        "schema_path = os.path.join(MODEL_DIR, \"feature_schema.json\")\n",
        "with open(schema_path, \"w\") as f:\n",
        "    json.dump(feature_schema, f, indent=2)\n",
        "\n",
        "readme_text = f\"\"\"\n",
        "============================================\n",
        "LightGBM 1D Inference \u2014 Deployment Notes\n",
        "============================================\n",
        "\n",
        "Artifacts:\n",
        "- Model:       {os.path.basename(model_path)}\n",
        "- Scaler:      {os.path.basename(scaler_path)}\n",
        "- Config:      {os.path.basename(config_path)}\n",
        "- Feature schema: feature_schema.json\n",
        "- Threshold:   {best_thr:.2f}\n",
        "- Predictions: test_predictions_1d.csv\n",
        "- Report:      lgbm_1d_day_by_day.txt\n",
        "\n",
        "Feature order (must match EXACTLY):\n",
        "{FEATURE_NAMES}\n",
        "\n",
        "Inference pipeline for your site:\n",
        "1) Parse raw input row:\n",
        "   - Parse 'raw_ohlcv_vec' -> [o,h,l,c,v]\n",
        "   - Parse 'iso_ohlc'      -> [iso_0..iso_3]\n",
        "   - Add one-hot: tf_1d=1, tf_4h=0\n",
        "   - Compute engineered features as in feature_schema.json\n",
        "   - Concatenate into a single 16-length vector in the listed order.\n",
        "\n",
        "2) Load scaler with joblib and call scaler.transform([vector]).\n",
        "3) Load model with joblib and call model.predict_proba(scaled)[0,1].\n",
        "4) If prob >= {best_thr:.2f} => predict UP (1); else DOWN (0).\n",
        "\n",
        "Notes:\n",
        "- Trained with class_weight='balanced'.\n",
        "- Scaler fit on the first 80% of pre-test (1d) training data.\n",
        "- Keep feature order and scaling identical for consistent results.\n",
        "\"\"\".strip()\n",
        "\n",
        "readme_path = os.path.join(MODEL_DIR, \"README_DEPLOY_1D.txt\")\n",
        "with open(readme_path, \"w\") as f:\n",
        "    f.write(readme_text)\n",
        "\n",
        "print(\"\ud83d\udce6 Deployment artifacts saved:\")\n",
        "print(\" -\", config_path)\n",
        "print(\" -\", schema_path)\n",
        "print(\" -\", readme_path)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 12: FINAL SUMMARY\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\ud83c\udfc6 LIGHTGBM_FINANCIAL 1D-ONLY \u2014 COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\" \u2022 Model dir:    {MODEL_DIR}\")\n",
        "print(f\" \u2022 Test window:  {test_start.date()} \u2192 {test_end.date()}\")\n",
        "print(f\" \u2022 Test candles: {len(X_test)}\")\n",
        "print(f\" \u2022 Test Acc/AUC: {test_acc:.4f} / {test_auc:.4f}\")\n",
        "print(f\" \u2022 Threshold:    {best_thr:.2f}\")\n",
        "print(\n",
        "    \" \u2022 Saved files:  \"\n",
        "    + \", \".join(\n",
        "        [\n",
        "            os.path.basename(model_path),\n",
        "            os.path.basename(scaler_path),\n",
        "            \"deployment_config.json\",\n",
        "            \"feature_schema.json\",\n",
        "            \"README_DEPLOY_1D.txt\",\n",
        "            os.path.basename(pred_csv_path),\n",
        "            os.path.basename(report_path),\n",
        "            \"results_1d_only.json\",\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}