{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWcUbV_uXjyr",
        "outputId": "5747d98f-9bf7-4d43-935b-65fec7067c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Setting up dependencies...\n",
            "✅ Core dependencies already available\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted (Colab environment)\n",
            "✅ Model directory: /content/drive/MyDrive/daygent_v1_models/gb_4h\n",
            "✅ Data directory:  /content/drive/MyDrive/daygent_v1_models/spy_data_export\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 1: CROSS-PLATFORM DEPENDENCY MANAGEMENT\n",
        "# ========================================\n",
        "print(\"🔧 Setting up dependencies...\")\n",
        "\n",
        "# Cross-platform dependency installation\n",
        "try:\n",
        "    import pandas, numpy, sklearn, xgboost, matplotlib, seaborn, joblib, tqdm\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import joblib\n",
        "    import lightgbm as lgb  # not used, keeps env parity\n",
        "    print(\"✅ Core dependencies already available\")\n",
        "except ImportError as e:\n",
        "    print(f\"Installing missing dependencies: {e}\")\n",
        "    import sys, subprocess\n",
        "    pkgs = [\n",
        "        'pandas','numpy','scikit-learn','xgboost','lightgbm',\n",
        "        'matplotlib','seaborn','joblib','tqdm','pyarrow'\n",
        "    ]\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + pkgs)\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    import joblib\n",
        "    print(\"✅ Dependencies installed\")\n",
        "\n",
        "# Try to mount Google Drive if available (Colab environment)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    BASE_DIR = '/content/drive/MyDrive/daygent_v1_models'  # same base folder as your other notebooks\n",
        "    print(\"✅ Google Drive mounted (Colab environment)\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    BASE_DIR = './daygent_v1_models'\n",
        "    print(\"✅ Local environment detected\")\n",
        "\n",
        "# Core imports\n",
        "import os, warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'spy_data_export')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'gb_4h')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"✅ Model directory: {MODEL_DIR}\")\n",
        "print(f\"✅ Data directory:  {DATA_DIR}\")\n",
        "\n",
        "# Determinism (GB is deterministic with fixed params; this removes any stray numpy randomness)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 2: LOAD 1D AND 4H DATA (FOR OVERLAP TEST PERIOD)\n",
        "# ========================================\n",
        "print(\"\\n📊 Loading 1D and 4H timeframe data...\")\n",
        "\n",
        "TIMEFRAMES_ORDERED = ['1d', '4h']\n",
        "raw_data = {}\n",
        "\n",
        "for tf in TIMEFRAMES_ORDERED:\n",
        "    csv_file = os.path.join(DATA_DIR, f'spy_{tf}.csv')\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise FileNotFoundError(f\"❌ {csv_file} not found!\")\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    raw_data[tf] = df\n",
        "    print(f\"✅ Loaded {tf} data: {len(df):,} candles\")\n",
        "    print(f\"📅 {tf} range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzSLni6aX9mk",
        "outputId": "c2400b73-0064-4bf0-8c27-7f0a880603b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Loading 1D and 4H timeframe data...\n",
            "✅ Loaded 1d data: 2,547 candles\n",
            "📅 1d range: 2014-12-23 14:30:00+00:00 to 2025-02-07 14:30:00+00:00\n",
            "✅ Loaded 4h data: 3,058 candles\n",
            "📅 4h range: 2019-01-07 14:30:00+00:00 to 2025-02-10 14:30:00+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 3: DEFINE TEST PERIOD (LAST 35 COMMON TRADING DAYS)\n",
        "# ========================================\n",
        "latest_start = max(raw_data['1d']['timestamp'].min(), raw_data['4h']['timestamp'].min())\n",
        "earliest_end  = min(raw_data['1d']['timestamp'].max(), raw_data['4h']['timestamp'].max())\n",
        "\n",
        "# common trading days across 1d/4h\n",
        "common_dates = set(\n",
        "    raw_data['1d'][(raw_data['1d']['timestamp'] >= latest_start) &\n",
        "                   (raw_data['1d']['timestamp'] <= earliest_end)]['timestamp'].dt.date.unique()\n",
        ")\n",
        "common_dates &= set(\n",
        "    raw_data['4h'][(raw_data['4h']['timestamp'] >= latest_start) &\n",
        "                   (raw_data['4h']['timestamp'] <= earliest_end)]['timestamp'].dt.date.unique()\n",
        ")\n",
        "\n",
        "all_days = sorted(common_dates)\n",
        "TEST_DAYS = min(35, len(all_days))\n",
        "selected_days = all_days[-TEST_DAYS:]\n",
        "\n",
        "test_start = pd.Timestamp.combine(selected_days[0],  pd.Timestamp.min.time()).tz_localize('UTC')\n",
        "test_end   = pd.Timestamp.combine(selected_days[-1], pd.Timestamp.max.time()).tz_localize('UTC')\n",
        "\n",
        "print(f\"\\n🎯 Test period: {test_start.date()} → {test_end.date()} ({TEST_DAYS} trading days)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Iwn2CjYABc",
        "outputId": "8ddd6d81-ff4a-4eed-a4a4-e5a1099808ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Test period: 2024-12-17 → 2025-02-07 (35 trading days)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 4: FEATURE EXTRACTION (16-FEATURE CONTRACT)\n",
        "# ========================================\n",
        "def parse_vector_column(vector_str):\n",
        "    \"\"\"Parse vector string to numpy array.\"\"\"\n",
        "    if pd.isna(vector_str) or vector_str is None:\n",
        "        return None\n",
        "    if isinstance(vector_str, str):\n",
        "        s = vector_str.strip('[]\"')\n",
        "        try:\n",
        "            return np.array([float(x.strip()) for x in s.split(',')])\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return np.array(vector_str)\n",
        "\n",
        "FEATURE_NAMES = [\n",
        "    'raw_o','raw_h','raw_l','raw_c','raw_v',\n",
        "    'iso_0','iso_1','iso_2','iso_3',\n",
        "    'tf_1d','tf_4h',\n",
        "    'hl_range','price_change','upper_shadow','lower_shadow','volume_m'\n",
        "]\n",
        "\n",
        "def build_feature_vector(raw_ohlcv, iso_ohlc, tf, tf_list):\n",
        "    \"\"\"Build 16-feature vector (5 raw + 4 iso + 2 one-hot + 5 engineered).\"\"\"\n",
        "    o, h, l, c, v = raw_ohlcv\n",
        "    features = list(raw_ohlcv)  # 5\n",
        "    features.extend(list(iso_ohlc))  # 4\n",
        "    features.extend([1 if tf == t else 0 for t in tf_list])  # 2\n",
        "    features.extend([\n",
        "        (h - l) / c if c else 0,  # hl_range\n",
        "        (c - o) / o if o else 0,  # price_change\n",
        "        (h - c) / c if c else 0,  # upper_shadow\n",
        "        (c - l) / c if c else 0,  # lower_shadow\n",
        "        v / 1_000_000,            # volume_m\n",
        "    ])  # 5\n",
        "    return np.array(features, dtype=float)\n",
        "\n",
        "def extract_features_4h(row):\n",
        "    raw_ohlcv = parse_vector_column(row.get('raw_ohlcv_vec'))\n",
        "    iso_ohlc  = parse_vector_column(row.get('iso_ohlc'))\n",
        "    future    = row.get('future')\n",
        "    if raw_ohlcv is None or iso_ohlc is None or pd.isna(future):\n",
        "        return None, None\n",
        "    if len(raw_ohlcv) != 5 or len(iso_ohlc) != 4:\n",
        "        return None, None\n",
        "    return build_feature_vector(raw_ohlcv, iso_ohlc, '4h', TIMEFRAMES_ORDERED), int(future)\n"
      ],
      "metadata": {
        "id": "bwEmoSomYB3k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 5: EXTRACT TRAIN/TEST FEATURES (W2-STYLE) & SCALE\n",
        "# ========================================\n",
        "print(\"\\n🔄 Extracting features from 4h data...\")\n",
        "\n",
        "df_4h = raw_data['4h']\n",
        "train_df = df_4h[df_4h['timestamp'] < test_start].copy()\n",
        "test_df  = df_4h[(df_4h['timestamp'] >= test_start) & (df_4h['timestamp'] <= test_end)].copy()\n",
        "\n",
        "print(f\"📊 Train samples (pre-test period): {len(train_df):,}\")\n",
        "print(f\"📊 Test samples (overlap window):  {len(test_df):,}\")\n",
        "\n",
        "# Train features\n",
        "X_train, y_train = [], []\n",
        "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Extracting 4h train features\"):\n",
        "    fv, lbl = extract_features_4h(row)\n",
        "    if fv is not None:\n",
        "        X_train.append(fv); y_train.append(lbl)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "print(f\"\\n✅ Training features extracted: {X_train.shape}\")\n",
        "if len(y_train):\n",
        "    print(f\"📊 Class distribution (train): {np.bincount(y_train)}\")\n",
        "\n",
        "# Test features + raw info for detailed report\n",
        "X_test, y_test, test_timestamps = [], [], []\n",
        "test_rows_info = []\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Extracting 4h test features\"):\n",
        "    fv, lbl = extract_features_4h(row)\n",
        "    if fv is not None:\n",
        "        X_test.append(fv); y_test.append(lbl); test_timestamps.append(row['timestamp'])\n",
        "        test_rows_info.append({\n",
        "            'timestamp': row['timestamp'],\n",
        "            'raw_ohlcv': parse_vector_column(row['raw_ohlcv_vec']),\n",
        "            'iso_ohlc':  parse_vector_column(row['iso_ohlc']),\n",
        "            'future': int(row['future']),\n",
        "            'feature_vector': fv\n",
        "        })\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "print(f\"📊 Test features extracted: {X_test.shape}\")\n",
        "\n",
        "# Scale using the FIRST 80% of the pre-test data (exactly like the w2 optimizer did)\n",
        "scaler = StandardScaler()\n",
        "split_ix = int(len(X_train) * 0.8)\n",
        "scaler.fit(X_train[:split_ix])\n",
        "\n",
        "X_scaled = scaler.transform(X_train)\n",
        "X_tr = X_scaled[:split_ix]\n",
        "X_val = X_scaled[split_ix:]  # kept for sanity metrics (not used for calibration or refit)\n",
        "y_tr = y_train[:split_ix]\n",
        "y_val = y_train[split_ix:]\n",
        "\n",
        "print(f\"📊 Train slice used for fitting GB: {X_tr.shape}\")\n",
        "print(f\"📊 Held-out in-sample val slice : {X_val.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP60nxHkYEe3",
        "outputId": "3285c7fd-ba7b-421f-be63-eeccc753b6de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Extracting features from 4h data...\n",
            "📊 Train samples (pre-test period): 2,988\n",
            "📊 Test samples (overlap window):  69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting 4h train features: 100%|██████████| 2988/2988 [00:00<00:00, 14147.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Training features extracted: (2988, 16)\n",
            "📊 Class distribution (train): [1363 1625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting 4h test features: 100%|██████████| 69/69 [00:00<00:00, 9655.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Test features extracted: (69, 16)\n",
            "📊 Train slice used for fitting GB: (2390, 16)\n",
            "📊 Held-out in-sample val slice : (598, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 6: TRAIN GRADIENT BOOSTING ON TRAIN SLICE ONLY (W2-STYLE)\n",
        "# ========================================\n",
        "print(\"\\n🚀 Training GradientBoosting (4h) — train slice only, no threshold calibration, no refit\")\n",
        "\n",
        "gb_params = {\n",
        "    'n_estimators'     : 75,\n",
        "    'max_depth'        : 9,\n",
        "    'learning_rate'    : 0.12,\n",
        "    'subsample'        : 0.85,\n",
        "    'min_samples_split': 12,\n",
        "    'min_samples_leaf' : 1,\n",
        "    'random_state'     : 42,\n",
        "}\n",
        "\n",
        "gb = GradientBoostingClassifier(**gb_params)\n",
        "\n",
        "# Balanced weights on the train slice only (matches optimizer behavior)\n",
        "sw_tr = compute_sample_weight('balanced', y_tr) if len(y_tr) else None\n",
        "gb.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
        "\n",
        "# Sanity: in-sample val metrics at default 0.50\n",
        "val_acc = float('nan'); val_auc = float('nan')\n",
        "if len(X_val):\n",
        "    val_pred  = gb.predict(X_val)\n",
        "    val_acc   = accuracy_score(y_val, val_pred)\n",
        "    val_proba = gb.predict_proba(X_val)[:, 1]\n",
        "    if len(np.unique(y_val)) == 2:\n",
        "        val_auc = roc_auc_score(y_val, val_proba)\n",
        "\n",
        "print(f\"✅ In-sample val accuracy (0.50): {val_acc:.4f}\")\n",
        "print(f\"✅ In-sample val AUC          : {val_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "138VlyJLYK5P",
        "outputId": "24113fbb-b490-4464-eea6-71859802b15c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Training GradientBoosting (4h) — train slice only, no threshold calibration, no refit\n",
            "✅ In-sample val accuracy (0.50): 0.5050\n",
            "✅ In-sample val AUC          : 0.5093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 7: TEST + DETAILED DAY-BY-DAY / PREDICTION-BY-PREDICTION ANALYSIS\n",
        "# ========================================\n",
        "print(f\"\\n🧪 Testing on isolated {len(selected_days)}-day overlap period (4h)...\")\n",
        "\n",
        "# Scale test using SAME scaler (fitted on first 80% of pre-test)\n",
        "X_test_scaled = scaler.transform(X_test) if len(X_test) else np.empty((0, X_tr.shape[1]))\n",
        "\n",
        "# Predict with the trained model (default threshold 0.50 via .predict)\n",
        "test_pred  = gb.predict(X_test_scaled) if len(X_test_scaled) else np.array([])\n",
        "test_proba = gb.predict_proba(X_test_scaled)[:, 1] if len(X_test_scaled) else np.array([])\n",
        "\n",
        "# Metrics\n",
        "test_acc = accuracy_score(y_test, test_pred) if len(test_pred) else float('nan')\n",
        "test_auc = roc_auc_score(y_test, test_proba) if (len(test_proba) and len(np.unique(y_test))==2) else float('nan')\n",
        "\n",
        "print(\"\\n🎯 TEST RESULTS (4h, w2-style):\")\n",
        "print(f\"✅ Test Accuracy (0.50): {test_acc:.4f}\")\n",
        "print(f\"✅ Test AUC            : {test_auc:.4f}\")\n",
        "if len(test_pred):\n",
        "    print(f\"📊 Test predictions: {np.bincount(test_pred)}\")\n",
        "    print(f\"📊 Actual labels :  {np.bincount(y_test)}\")\n",
        "\n",
        "# Build detailed per-prediction table\n",
        "FEATURE_INDEX = {name: i for i, name in enumerate(FEATURE_NAMES)}\n",
        "THRESHOLD_USED = 0.50\n",
        "\n",
        "records = []\n",
        "for i, info in enumerate(test_rows_info):\n",
        "    ts   = info['timestamp']\n",
        "    fv   = info['feature_vector']\n",
        "    raw  = info['raw_ohlcv']\n",
        "    iso  = info['iso_ohlc']\n",
        "    true = info['future']\n",
        "\n",
        "    proba = float(test_proba[i])\n",
        "    pred  = int(test_pred[i])\n",
        "    correct = bool(pred == true)\n",
        "    margin = proba - THRESHOLD_USED\n",
        "\n",
        "    rec = {\n",
        "        'candle_index_in_test': i + 1,\n",
        "        'timestamp_utc': ts,\n",
        "        'date_utc': ts.date(),\n",
        "        'pred_prob_up': proba,\n",
        "        'pred_label': int(pred),      # 1=up, 0=down\n",
        "        'true_label': int(true),\n",
        "        'correct': correct,\n",
        "        'threshold_used': THRESHOLD_USED,\n",
        "        'decision_margin': margin,\n",
        "\n",
        "        # Raw 4h OHLCV & ISO\n",
        "        'raw_o': raw[0], 'raw_h': raw[1], 'raw_l': raw[2], 'raw_c': raw[3], 'raw_v': raw[4],\n",
        "        'iso_0': iso[0], 'iso_1': iso[1], 'iso_2': iso[2], 'iso_3': iso[3],\n",
        "\n",
        "        # Engineered features from fv\n",
        "        'tf_1d': fv[FEATURE_INDEX['tf_1d']],\n",
        "        'tf_4h': fv[FEATURE_INDEX['tf_4h']],\n",
        "        'hl_range': fv[FEATURE_INDEX['hl_range']],\n",
        "        'price_change': fv[FEATURE_INDEX['price_change']],\n",
        "        'upper_shadow': fv[FEATURE_INDEX['upper_shadow']],\n",
        "        'lower_shadow': fv[FEATURE_INDEX['lower_shadow']],\n",
        "        'volume_m': fv[FEATURE_INDEX['volume_m']],\n",
        "    }\n",
        "    records.append(rec)\n",
        "\n",
        "pred_df = pd.DataFrame.from_records(records).sort_values(['date_utc','timestamp_utc']).reset_index(drop=True)\n",
        "\n",
        "# Save machine-friendly CSV\n",
        "pred_csv_path = os.path.join(MODEL_DIR, 'test_predictions_4h.csv')\n",
        "pred_df.to_csv(pred_csv_path, index=False)\n",
        "\n",
        "# Human-readable TXT report grouped by day\n",
        "txt_lines = []\n",
        "txt_lines.append(\"=\"*90)\n",
        "txt_lines.append(\"GRADIENT BOOSTING 4H — DETAILED DAY-BY-DAY / PREDICTION-BY-PREDICTION REPORT\")\n",
        "txt_lines.append(\"=\"*90)\n",
        "txt_lines.append(f\"Test period: {test_start.date()} → {test_end.date()}\")\n",
        "txt_lines.append(f\"Total test candles: {len(pred_df)}\")\n",
        "txt_lines.append(f\"Threshold used: {THRESHOLD_USED:.2f}\")\n",
        "txt_lines.append(f\"Overall Test Accuracy: {test_acc:.4f}\")\n",
        "txt_lines.append(f\"Overall Test AUC: {test_auc:.4f}\")\n",
        "txt_lines.append(\"\")\n",
        "\n",
        "for day in pred_df['date_utc'].unique():\n",
        "    day_block = pred_df[pred_df['date_utc'] == day]\n",
        "    correct_n = int(day_block['correct'].sum())\n",
        "    total_n   = len(day_block)\n",
        "    txt_lines.append(\"-\"*90)\n",
        "    txt_lines.append(f\"{day}  —  Day accuracy: {correct_n}/{total_n}  ({correct_n/total_n:.3f})\")\n",
        "    txt_lines.append(\"-\"*90)\n",
        "    for _, r in day_block.iterrows():\n",
        "        dir_word   = \"UP\" if r['pred_label'] == 1 else \"DOWN\"\n",
        "        truth_word = \"UP\" if r['true_label'] == 1 else \"DOWN\"\n",
        "        right_wrong = \"✅ CORRECT\" if r['correct'] else \"❌ WRONG\"\n",
        "        txt_lines.append(\n",
        "            f\"[{int(r['candle_index_in_test']):02d}] {r['timestamp_utc']}  \"\n",
        "            f\"pred={dir_word}  p_up={r['pred_prob_up']:.4f}  thr={r['threshold_used']:.2f}  \"\n",
        "            f\"margin={r['decision_margin']:.4f}  truth={truth_word}  → {right_wrong}\"\n",
        "        )\n",
        "        txt_lines.append(\n",
        "            f\"    OHLCV: O={r['raw_o']:.4f}, H={r['raw_h']:.4f}, L={r['raw_l']:.4f}, C={r['raw_c']:.4f}, V={r['raw_v']:.0f} | \"\n",
        "            f\"ISO: [{r['iso_0']:.4f}, {r['iso_1']:.4f}, {r['iso_2']:.4f}, {r['iso_3']:.4f}] | \"\n",
        "            f\"feats: hl={r['hl_range']:.4f}, dC={r['price_change']:.4f}, upSh={r['upper_shadow']:.4f}, \"\n",
        "            f\"loSh={r['lower_shadow']:.4f}, vol_m={r['volume_m']:.4f}\"\n",
        "        )\n",
        "    txt_lines.append(\"\")\n",
        "\n",
        "report_path = os.path.join(MODEL_DIR, 'gb_4h_day_by_day.txt')\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(\"\\n\".join(txt_lines))\n",
        "\n",
        "print(f\"\\n📝 Saved detailed TXT report to: {report_path}\")\n",
        "print(f\"🧾 Saved machine-readable predictions to: {pred_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsZk1MmZYNJm",
        "outputId": "710e46f1-e601-440d-d339-b6d53f03fcff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Testing on isolated 35-day overlap period (4h)...\n",
            "\n",
            "🎯 TEST RESULTS (4h, w2-style):\n",
            "✅ Test Accuracy (0.50): 0.6957\n",
            "✅ Test AUC            : 0.7052\n",
            "📊 Test predictions: [26 43]\n",
            "📊 Actual labels :  [29 40]\n",
            "\n",
            "📝 Saved detailed TXT report to: /content/drive/MyDrive/daygent_v1_models/gb_4h/gb_4h_day_by_day.txt\n",
            "🧾 Saved machine-readable predictions to: /content/drive/MyDrive/daygent_v1_models/gb_4h/test_predictions_4h.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 8: SAVE MODEL, SCALER, AND RESULTS\n",
        "# ========================================\n",
        "print(\"\\n💾 Saving model and results...\")\n",
        "\n",
        "model_path  = os.path.join(MODEL_DIR, 'gb_4h_w2_style.joblib')\n",
        "scaler_path = os.path.join(MODEL_DIR, 'scaler_4h_w2_style.joblib')\n",
        "joblib.dump(gb, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "def _to_py(v):\n",
        "    try:\n",
        "        if isinstance(v, (np.integer, np.int64, np.int32)):\n",
        "            return int(v)\n",
        "        if isinstance(v, (np.floating,)):\n",
        "            return float(v)\n",
        "        return v\n",
        "    except Exception:\n",
        "        return v\n",
        "\n",
        "results = {\n",
        "    'pipeline': 'w2-style: train on first 80% slice only, default threshold 0.50, no refit',\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'test_auc': float(test_auc),\n",
        "    'val_accuracy_0p50': float(val_acc) if not np.isnan(val_acc) else None,\n",
        "    'val_auc': float(val_auc) if not np.isnan(val_auc) else None,\n",
        "    'train_samples_used_for_fit': int(len(X_tr)),\n",
        "    'heldout_in_sample_val_samples': int(len(X_val)),\n",
        "    'test_samples': int(len(X_test)),\n",
        "    'feature_count': int(X_train.shape[1]) if X_train.ndim == 2 else 0,\n",
        "    'model_params': {k: _to_py(v) for k, v in gb_params.items()},\n",
        "    'feature_names': FEATURE_NAMES,\n",
        "    'report_txt': os.path.basename(report_path),\n",
        "    'predictions_csv': os.path.basename(pred_csv_path),\n",
        "    'model_path': os.path.basename(model_path),\n",
        "    'scaler_path': os.path.basename(scaler_path),\n",
        "    'threshold_used': 0.50,\n",
        "    'test_period': f\"{test_start.date()} to {test_end.date()}\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(os.path.join(MODEL_DIR, 'results_gb_4h_w2_style.json'), 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"✅ Model saved to: {model_path}\")\n",
        "print(f\"✅ Scaler saved to: {scaler_path}\")\n",
        "print(\"✅ Results JSON saved as: results_gb_4h_w2_style.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU7P4i4vYWN8",
        "outputId": "26ca5254-f965-4996-9c39-e9cabc806d75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Saving model and results...\n",
            "✅ Model saved to: /content/drive/MyDrive/daygent_v1_models/gb_4h/gb_4h_w2_style.joblib\n",
            "✅ Scaler saved to: /content/drive/MyDrive/daygent_v1_models/gb_4h/scaler_4h_w2_style.joblib\n",
            "✅ Results JSON saved as: results_gb_4h_w2_style.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# STEP 9: SAVE DEPLOYMENT ARTIFACTS (for your site)\n",
        "# ========================================\n",
        "import json\n",
        "from textwrap import dedent\n",
        "import os\n",
        "\n",
        "deployment_config = {\n",
        "    \"model_type\": \"GradientBoostingClassifier\",\n",
        "    \"timeframe\": \"4h\",\n",
        "    \"feature_contract_version\": \"v1\",\n",
        "    \"feature_names\": FEATURE_NAMES,\n",
        "    \"threshold_used\": 0.50,\n",
        "    \"artifact_paths\": {\n",
        "        \"model_joblib\": \"gb_4h_w2_style.joblib\",\n",
        "        \"scaler_joblib\": \"scaler_4h_w2_style.joblib\"\n",
        "    },\n",
        "    \"inference_notes\": {\n",
        "        \"scaling\": \"StandardScaler fitted on first 80% of pre-test 4h training data\",\n",
        "        \"one_hot\": {\"tf_1d\": 0, \"tf_4h\": 1},\n",
        "        \"expected_columns_in_csv\": [\"timestamp\", \"raw_ohlcv_vec\", \"iso_ohlc\", \"future\"]\n",
        "    },\n",
        "    \"gb_params\": results[\"model_params\"]\n",
        "}\n",
        "\n",
        "config_path = os.path.join(MODEL_DIR, \"deployment_config_4h.json\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(deployment_config, f, indent=2)\n",
        "\n",
        "feature_schema = {\n",
        "    \"raw_ohlcv_vec\": {\n",
        "        \"desc\": \"Stringified list of [open, high, low, close, volume]\",\n",
        "        \"len\": 5,\n",
        "        \"dtype\": \"float\"\n",
        "    },\n",
        "    \"iso_ohlc\": {\n",
        "        \"desc\": \"Stringified list of 4 ISO-normalized OHLC values\",\n",
        "        \"len\": 4,\n",
        "        \"dtype\": \"float\"\n",
        "    },\n",
        "    \"engineered\": [\n",
        "        \"hl_range=(H-L)/C\",\n",
        "        \"price_change=(C-O)/O\",\n",
        "        \"upper_shadow=(H-C)/C\",\n",
        "        \"lower_shadow=(C-L)/C\",\n",
        "        \"volume_m=V/1e6\"\n",
        "    ],\n",
        "    \"tf_one_hot\": {\"tf_1d\": 0, \"tf_4h\": 1}\n",
        "}\n",
        "\n",
        "schema_path = os.path.join(MODEL_DIR, \"feature_schema_4h.json\")\n",
        "with open(schema_path, \"w\") as f:\n",
        "    json.dump(feature_schema, f, indent=2)\n",
        "\n",
        "readme_text = dedent(f\"\"\"\n",
        "    ============================================\n",
        "    GradientBoosting 4H Inference — Deployment Notes\n",
        "    ============================================\n",
        "\n",
        "    Artifacts:\n",
        "    - Model:       {os.path.basename(model_path)}\n",
        "    - Scaler:      {os.path.basename(scaler_path)}\n",
        "    - Config:      {os.path.basename(config_path)}\n",
        "    - Feature schema: feature_schema_4h.json\n",
        "    - Predictions: test_predictions_4h.csv\n",
        "    - Report:      gb_4h_day_by_day.txt\n",
        "\n",
        "    Pipeline (w2-style):\n",
        "    • Train on first 80% slice of pre-test 4h data (after scaling with that same slice).\n",
        "    • No threshold calibration; default 0.50 is used.\n",
        "    • No refit on train+val.\n",
        "    • Inference uses scaler.transform then model.predict_proba(...)[1] and 0.50 cutoff.\n",
        "\n",
        "    Feature order (must match EXACTLY):\n",
        "    {FEATURE_NAMES}\n",
        "\n",
        "    Inference steps for your site:\n",
        "    1) Parse inputs:\n",
        "       - 'raw_ohlcv_vec' -> [o,h,l,c,v]\n",
        "       - 'iso_ohlc'      -> [iso_0..iso_3]\n",
        "       - one-hot: tf_1d=0, tf_4h=1\n",
        "       - engineered: hl_range, price_change, upper_shadow, lower_shadow, volume_m\n",
        "       - concatenate into a 16-length vector in the EXACT order above\n",
        "    2) Load scaler (joblib) and transform the vector.\n",
        "    3) Load model (joblib) and compute P(up) = predict_proba(...)[0,1].\n",
        "    4) Predict UP if P(up) >= 0.50 else DOWN.\n",
        "\n",
        "    Keep the feature order + scaling identical for consistent results.\n",
        "\"\"\").strip()\n",
        "\n",
        "readme_path = os.path.join(MODEL_DIR, \"README_DEPLOY_4H.txt\")\n",
        "with open(readme_path, \"w\") as f:\n",
        "    f.write(readme_text)\n",
        "\n",
        "print(\"📦 Deployment artifacts saved:\")\n",
        "print(\" -\", config_path)\n",
        "print(\" -\", schema_path)\n",
        "print(\" -\", readme_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwG3NlzRYabt",
        "outputId": "857f5498-4ee5-4189-e2c5-5349498f7f63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Deployment artifacts saved:\n",
            " - /content/drive/MyDrive/daygent_v1_models/gb_4h/deployment_config_4h.json\n",
            " - /content/drive/MyDrive/daygent_v1_models/gb_4h/feature_schema_4h.json\n",
            " - /content/drive/MyDrive/daygent_v1_models/gb_4h/README_DEPLOY_4H.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FINAL SUMMARY\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🏆 GRADIENT BOOSTING 4H — W2-STYLE COMPLETE (with deep analysis + deploy artifacts)\")\n",
        "print(\"=\"*70)\n",
        "print(f\" • Model dir:    {MODEL_DIR}\")\n",
        "print(f\" • Test window:  {test_start.date()} → {test_end.date()}\")\n",
        "print(f\" • Test candles: {len(X_test)}\")\n",
        "print(f\" • Test Acc/AUC: {test_acc:.4f} / {test_auc:.4f}\")\n",
        "print(f\" • Threshold:    0.50\")\n",
        "print(f\" • Saved files:  gb_4h_w2_style.joblib, scaler_4h_w2_style.joblib,\")\n",
        "print(f\"                 deployment_config_4h.json, feature_schema_4h.json, README_DEPLOY_4H.txt,\")\n",
        "print(f\"                 test_predictions_4h.csv, gb_4h_day_by_day.txt, results_gb_4h_w2_style.json\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okTu4YOXYeGC",
        "outputId": "36832e07-19de-41dc-887d-7521b586ade8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🏆 GRADIENT BOOSTING 4H — W2-STYLE COMPLETE (with deep analysis + deploy artifacts)\n",
            "======================================================================\n",
            " • Model dir:    /content/drive/MyDrive/daygent_v1_models/gb_4h\n",
            " • Test window:  2024-12-17 → 2025-02-07\n",
            " • Test candles: 69\n",
            " • Test Acc/AUC: 0.6957 / 0.7052\n",
            " • Threshold:    0.50\n",
            " • Saved files:  gb_4h_w2_style.joblib, scaler_4h_w2_style.joblib,\n",
            "                 deployment_config_4h.json, feature_schema_4h.json, README_DEPLOY_4H.txt,\n",
            "                 test_predictions_4h.csv, gb_4h_day_by_day.txt, results_gb_4h_w2_style.json\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}