{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "LightGBM 1D v0 \u2014 Training Script\n",
        "\n",
        "Purpose:\n",
        "- Train a LightGBM classifier on daily SPY bars using time-ordered CV,\n",
        "  recency weights, EV-based thresholding, final refit with early stopping,\n",
        "  and OOS evaluation. Saves model, scaler, reports, and OOS predictions.\n",
        "\n",
        "CLI arguments:\n",
        "- None. This script uses the constants in CELL 1 for configuration.\n",
        "  To change behavior, edit those constants directly.\n",
        "\n",
        "Defaults (editable in CELL 1):\n",
        "- Input CSV: daygent/data/spy_1d.csv (columns: symbol, timestamp, raw_ohlcv_vec, iso_ohlc, future)\n",
        "- Costs (round-trip): 6 bps (0.0006)\n",
        "- Threshold sweep: 0.30 \u2192 0.80 (step 0.01), EV floor = 3 bps\n",
        "- Recency weighting: tau_bars=180 (by bars)\n",
        "- Artifacts:\n",
        "  - daygent/models/lightgbm_1d_v0.joblib\n",
        "  - daygent/models/scaler_1d_v0.joblib\n",
        "  - daygent/reports/lightgbm_1d_v0_threshold.json\n",
        "  - daygent/reports/lightgbm_1d_v0_metrics.json\n",
        "  - daygent/preds/lightgbm_1d_v0_oos.csv\n",
        "\n",
        "Drive-aware behavior:\n",
        "- If running in Colab (or if a local ./daygent_v1_models exists), the script\n",
        "  will use BASE_DIR = '/content/drive/MyDrive/daygent_v1_models' (Colab)\n",
        "  or './daygent_v1_models' (local) for:\n",
        "    - Combined data: {BASE_DIR}/combined_spy_data/combined_spy_1d.csv (single source of truth)\n",
        "    - Artifacts: {BASE_DIR}/lgbm_1d_v0\n",
        "\"\"\"\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 1: CROSS-PLATFORM DEPENDENCY MANAGEMENT\n",
        "print(\"\ud83d\udd27 Setting up dependencies...\")\n",
        "\n",
        "# Cross-platform dependency installation\n",
        "try:\n",
        "    import pandas, numpy, sklearn, matplotlib, seaborn, joblib, tqdm\n",
        "    import lightgbm as lgb\n",
        "    print(\"\u2705 Core dependencies already available\")\n",
        "except ImportError as e:\n",
        "    print(f\"Installing missing dependencies: {e}\")\n",
        "    import sys, subprocess\n",
        "    pkgs = ['pandas', 'numpy', 'scikit-learn', 'lightgbm',\n",
        "            'matplotlib', 'seaborn', 'joblib', 'tqdm', 'pyarrow']\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + pkgs)\n",
        "    import lightgbm as lgb\n",
        "    print(\"\u2705 Dependencies installed\")\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 2: CONFIGURATION & CONSTANTS\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Try to mount Google Drive if available (Colab environment)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    print(\"\u2705 Google Drive mounted (Colab environment)\")\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    print(\"\u2705 Local environment detected\")\n",
        "\n",
        "# Prefer Drive-style BASE_DIR if available; else use local daygent/* folders\n",
        "if IS_COLAB:\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/daygent_v1_models')  # <\u2014 your new base folder\n",
        "else:\n",
        "    BASE_DIR = Path('./daygent_v1_models')\n",
        "\n",
        "print(f\"\u2705 Base directory: {BASE_DIR}\")\n",
        "\n",
        "# Local fallback dirs for artifacts\n",
        "DATA_DIR = Path(\"daygent/data\")\n",
        "MODELS_DIR = Path(\"daygent/models\")\n",
        "REPORTS_DIR = Path(\"daygent/reports\")\n",
        "PREDS_DIR = Path(\"daygent/preds\")\n",
        "\n",
        "for d in [DATA_DIR, MODELS_DIR, REPORTS_DIR, PREDS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Combined data dir (Drive) \u2013 single source of truth\n",
        "COMBINED_DIR_DRIVE = BASE_DIR / 'combined_spy_data'\n",
        "COMBINED_1D_CSV_DRIVE = COMBINED_DIR_DRIVE / 'combined_spy_1d.csv'\n",
        "print(f\"\u2705 Combined data path: {COMBINED_1D_CSV_DRIVE}\")\n",
        "\n",
        "# If Drive-style artifacts dir exists or we're on Colab, save artifacts there\n",
        "ARTIFACT_DIR = None\n",
        "if IS_COLAB or BASE_DIR.exists():\n",
        "    ARTIFACT_DIR = BASE_DIR / 'lgbm_1d_v0'\n",
        "    ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    MODEL_PATH = ARTIFACT_DIR / \"lightgbm_1d_v0.joblib\"\n",
        "    SCALER_PATH = ARTIFACT_DIR / \"scaler_1d_v0.joblib\"\n",
        "    THRESHOLD_JSON = ARTIFACT_DIR / \"lightgbm_1d_v0_threshold.json\"\n",
        "    METRICS_JSON = ARTIFACT_DIR / \"lightgbm_1d_v0_metrics.json\"\n",
        "    OOS_PRED_CSV = ARTIFACT_DIR / \"lightgbm_1d_v0_oos.csv\"\n",
        "    print(f\"\u2705 Model directory: {ARTIFACT_DIR}\")\n",
        "\n",
        "# Trading/eval parameters\n",
        "COST_ROUNDTRIP = 0.0006  # 6 bps default; can be replaced by per-day vector later\n",
        "THRESHOLDS = np.round(np.arange(0.30, 0.801, 0.01), 2)\n",
        "EV_FLOOR = 0.0003  # 3 bps minimum EV in sweep\n",
        "MIN_TRADES_PER_FOLD = 30  # guard against thin selections\n",
        "TAU_BARS = 180  # recency weighting in number of bars (not calendar days)\n",
        "\n",
        "# Feature contract (ordered)\n",
        "EXPECTED_FEATS = [\n",
        "    \"raw_o\",\"raw_h\",\"raw_l\",\"raw_c\",\"raw_v\",\n",
        "    \"iso_ohlc_o\",\"iso_ohlc_h\",\"iso_ohlc_l\",\"iso_ohlc_c\",\n",
        "    \"tf_1d\",\"tf_4h\",\n",
        "    \"hl_range\",\"price_change\",\"upper_shadow\",\"lower_shadow\",\"volume_m\"\n",
        "]\n",
        "\n",
        "if 'ARTIFACT_DIR' in globals() and ARTIFACT_DIR is not None:\n",
        "    # Already set to Drive above\n",
        "    pass\n",
        "else:\n",
        "    # Local single artifact directory for all outputs\n",
        "    ARTIFACT_DIR = Path(\"daygent/lgbm_1d_v0\")\n",
        "    ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    MODEL_PATH = ARTIFACT_DIR / \"lightgbm_1d_v0.joblib\"\n",
        "    SCALER_PATH = ARTIFACT_DIR / \"scaler_1d_v0.joblib\"\n",
        "    THRESHOLD_JSON = ARTIFACT_DIR / \"lightgbm_1d_v0_threshold.json\"\n",
        "    METRICS_JSON = ARTIFACT_DIR / \"lightgbm_1d_v0_metrics.json\"\n",
        "    OOS_PRED_CSV = ARTIFACT_DIR / \"lightgbm_1d_v0_oos.csv\"\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 3: UTILS (RECENCY WEIGHTING, METRICS, SAFE CAST)\n",
        "def compute_recency_weights_by_bars(num_bars: int, tau_bars: int) -> np.ndarray:\n",
        "    \"\"\"Exponential recency weights by index distance (bars), not calendar days.\n",
        "\n",
        "    Newest bar gets weight=1; weight decays backwards with tau_bars.\n",
        "    \"\"\"\n",
        "    if num_bars <= 0:\n",
        "        return np.array([])\n",
        "    idx = np.arange(num_bars)\n",
        "    delta = idx - idx.max()\n",
        "    return np.exp(delta / float(tau_bars))\n",
        "\n",
        "\n",
        "def ev_sweep(prob_up: np.ndarray,\n",
        "             ret_next: np.ndarray,\n",
        "             thresholds: np.ndarray,\n",
        "             cost: float,\n",
        "             min_trades: int = 0,\n",
        "             ev_floor: float = -np.inf,\n",
        "             cost_vec: np.ndarray | None = None) -> dict:\n",
        "    \"\"\"Sweep thresholds with guards; pick the one with max EV per trade.\n",
        "\n",
        "    Guards:\n",
        "      - require at least min_trades\n",
        "      - require EV >= ev_floor\n",
        "    \"\"\"\n",
        "    best = {\n",
        "        \"best_theta\": None,\n",
        "        \"best_ev\": -np.inf,\n",
        "        \"best_hit_net\": np.nan,\n",
        "        \"best_hit_gross\": np.nan,\n",
        "        \"best_trades\": 0,\n",
        "    }\n",
        "    for th in thresholds:\n",
        "        mask = prob_up >= th\n",
        "        n = int(mask.sum())\n",
        "        if n < min_trades:\n",
        "            continue\n",
        "        if cost_vec is not None:\n",
        "            r_net = ret_next[mask] - cost_vec[mask]\n",
        "        else:\n",
        "            r_net = ret_next[mask] - cost\n",
        "        ev = float(np.mean(r_net))\n",
        "        if ev < ev_floor:\n",
        "            continue\n",
        "        hit_net = float(np.mean(r_net > 0))\n",
        "        hit_gross = float(np.mean(ret_next[mask] > 0))\n",
        "        if ev > best[\"best_ev\"]:\n",
        "            best.update({\n",
        "                \"best_theta\": float(th),\n",
        "                \"best_ev\": ev,\n",
        "                \"best_hit_net\": hit_net,\n",
        "                \"best_hit_gross\": hit_gross,\n",
        "                \"best_trades\": n,\n",
        "            })\n",
        "    return best\n",
        "\n",
        "\n",
        "def safe_float(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 4: LOAD DATA & BUILD FEATURES/TARGETS (21-FEATURE CONTRACT)\n",
        "def build_features_targets(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
        "    \"\"\"Build feature vector (len=16) + binary label + next-day log return + dates.\n",
        "\n",
        "    Input columns expected: symbol, timestamp, raw_ohlcv_vec, iso_ohlc, future\n",
        "    Feature order (EXPECTED_FEATS):\n",
        "      raw_o, raw_h, raw_l, raw_c, raw_v,\n",
        "      iso_ohlc_o, iso_ohlc_h, iso_ohlc_l, iso_ohlc_c,\n",
        "      tf_1d, tf_4h,\n",
        "      hl_range, price_change, upper_shadow, lower_shadow, volume_m\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # Sort & sanity\n",
        "    # Normalize and sort strictly by known 'timestamp' column\n",
        "    if \"timestamp\" not in df.columns:\n",
        "        raise ValueError(\"Input CSV must include 'timestamp' column.\")\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]).dt.tz_localize(None)\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    df[\"date\"] = df[\"timestamp\"]  # maintain compatibility downstream\n",
        "\n",
        "    if not df['date'].is_monotonic_increasing:\n",
        "        raise ValueError(\"Dates not strictly increasing before features.\")\n",
        "    if df['date'].duplicated().any():\n",
        "        raise ValueError(\"Duplicate dates found before feature construction.\")\n",
        "\n",
        "    # Basic checks\n",
        "    required = [\"raw_ohlcv_vec\", \"iso_ohlc\", \"future\"]\n",
        "    for col in required:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "    # Raw features\n",
        "    feat = pd.DataFrame(index=df.index)\n",
        "    # Parse raw_ohlcv_vec column which is a stringified list [o,h,l,c,v]\n",
        "    def _parse_vec(s):\n",
        "        if pd.isna(s):\n",
        "            return None\n",
        "        if isinstance(s, (list, tuple, np.ndarray)):\n",
        "            return list(s)\n",
        "        try:\n",
        "            s = str(s).strip().strip('[]')\n",
        "            return [float(x.strip()) for x in s.split(',')]\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    ohlcv = df[\"raw_ohlcv_vec\"].apply(_parse_vec)\n",
        "    parts = ohlcv.apply(lambda x: x if x and len(x) == 5 else [np.nan]*5)\n",
        "    feat[\"raw_o\"] = parts.apply(lambda x: x[0])\n",
        "    feat[\"raw_h\"] = parts.apply(lambda x: x[1])\n",
        "    feat[\"raw_l\"] = parts.apply(lambda x: x[2])\n",
        "    feat[\"raw_c\"] = parts.apply(lambda x: x[3])\n",
        "    feat[\"raw_v\"] = parts.apply(lambda x: x[4])\n",
        "\n",
        "    # Engineered simple ratios base fields\n",
        "    c = feat[\"raw_c\"].astype(float)\n",
        "    o = feat[\"raw_o\"].astype(float)\n",
        "    h = feat[\"raw_h\"].astype(float)\n",
        "    l = feat[\"raw_l\"].astype(float)\n",
        "    v = feat[\"raw_v\"].astype(float)\n",
        "\n",
        "    eps = 1e-12\n",
        "\n",
        "    # ISO OHLC comes as a stringified list [iso_o, iso_h, iso_l, iso_c]\n",
        "    def _parse_iso(s):\n",
        "        if pd.isna(s):\n",
        "            return None\n",
        "        if isinstance(s, (list, tuple, np.ndarray)):\n",
        "            return list(s)\n",
        "        try:\n",
        "            s = str(s).strip().strip('[]')\n",
        "            return [float(x.strip()) for x in s.split(',')]\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    iso = df[\"iso_ohlc\"].apply(_parse_iso)\n",
        "    iso_parts = iso.apply(lambda x: x if x and len(x) == 4 else [0.0]*4)\n",
        "    feat[\"iso_ohlc_o\"] = iso_parts.apply(lambda x: x[0])\n",
        "    feat[\"iso_ohlc_h\"] = iso_parts.apply(lambda x: x[1])\n",
        "    feat[\"iso_ohlc_l\"] = iso_parts.apply(lambda x: x[2])\n",
        "    feat[\"iso_ohlc_c\"] = iso_parts.apply(lambda x: x[3])\n",
        "\n",
        "    # iso_ohlcv (5)\n",
        "    vv = np.log1p(v)\n",
        "    m5 = (o + h + l + c + vv) / 5.0\n",
        "    s5 = np.sqrt(((o - m5) ** 2 + (h - m5) ** 2 + (l - m5) ** 2 + (c - m5) ** 2 + (vv - m5) ** 2) / 5.0)\n",
        "    s5_safe = s5.mask(s5 <= 0, 1.0)\n",
        "\n",
        "    feat[\"iso_ohlcv_o\"] = ((o - m5) / s5_safe).where(s5 > 0, 0.0)\n",
        "    feat[\"iso_ohlcv_h\"] = ((h - m5) / s5_safe).where(s5 > 0, 0.0)\n",
        "    feat[\"iso_ohlcv_l\"] = ((l - m5) / s5_safe).where(s5 > 0, 0.0)\n",
        "    feat[\"iso_ohlcv_c\"] = ((c - m5) / s5_safe).where(s5 > 0, 0.0)\n",
        "    feat[\"iso_ohlcv_v\"] = ((vv - m5) / s5_safe).where(s5 > 0, 0.0)\n",
        "\n",
        "    # Timeframe one-hot\n",
        "    feat[\"tf_1d\"] = 1.0\n",
        "    feat[\"tf_4h\"] = 0.0\n",
        "\n",
        "    # Engineered simple ratios\n",
        "    feat[\"hl_range\"] = (h - l) / (c.replace(0, np.nan) + eps)\n",
        "    feat[\"price_change\"] = (c - o) / (o.replace(0, np.nan) + eps)\n",
        "    feat[\"upper_shadow\"] = (h - c) / (c.replace(0, np.nan) + eps)\n",
        "    feat[\"lower_shadow\"] = (c - l) / (c.replace(0, np.nan) + eps)\n",
        "    feat[\"volume_m\"] = v / 1_000_000.0\n",
        "\n",
        "    # Targets\n",
        "    # Labels from provided 'future' and returns from raw close\n",
        "    ret_next = np.log(c.shift(-1) / c)\n",
        "    if 'future' in df.columns:\n",
        "        y_cls = df['future'].astype(int)\n",
        "    else:\n",
        "        y_cls = (ret_next > 0).astype(int)\n",
        "\n",
        "    # Drop last row (no next-day)\n",
        "    mask = ret_next.notna()\n",
        "    feat = feat[mask].reset_index(drop=True)\n",
        "    y_cls = y_cls[mask].reset_index(drop=True)\n",
        "    ret_next = ret_next[mask].reset_index(drop=True)\n",
        "    dates = df.loc[mask, \"date\"].reset_index(drop=True)\n",
        "\n",
        "    # Enforce expected feature contract/order\n",
        "    feat = feat[EXPECTED_FEATS]\n",
        "\n",
        "    # Sanity: ensure no NaNs/Infs\n",
        "    vals = feat.values\n",
        "    if not np.isfinite(vals).all():\n",
        "        raise ValueError(\"NaNs or Infs detected in features after construction.\")\n",
        "\n",
        "    return feat, y_cls, ret_next, dates\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 5: 5-FOLD TIME-SERIES CV WITH RECENCY WEIGHTS & EV THRESHOLDING\n",
        "def run_time_series_cv(X: np.ndarray,\n",
        "                       y: np.ndarray,\n",
        "                       r: np.ndarray,\n",
        "                       dates: pd.Series,\n",
        "                       cutoff_date: pd.Timestamp) -> tuple[float, list, int]:\n",
        "    \"\"\"Run 5-fold TimeSeriesSplit on in-sample portion (<= cutoff).\n",
        "\n",
        "    Returns:\n",
        "      - theta_deploy (median of fold-best thresholds)\n",
        "      - fold_stats (list of dicts)\n",
        "      - best_iter_median (median of best_iteration_ across folds)\n",
        "    \"\"\"\n",
        "    in_sample_idx = np.where(dates.values.astype(\"datetime64[ns]\") <= np.datetime64(cutoff_date))[0]\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    fold_stats = []\n",
        "    fold_thetas = []\n",
        "    best_iters = []\n",
        "\n",
        "    for k, (tr_rel, va_rel) in enumerate(tscv.split(in_sample_idx), start=1):\n",
        "        tr_idx = in_sample_idx[tr_rel]\n",
        "        va_idx = in_sample_idx[va_rel]\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "        X_va, y_va = X[va_idx], y[va_idx]\n",
        "        r_va = r[va_idx]\n",
        "        d_tr = dates.iloc[tr_idx]\n",
        "\n",
        "        # Per-fold scaler\n",
        "        scaler = StandardScaler()\n",
        "        X_trs = scaler.fit_transform(X_tr)\n",
        "        X_vas = scaler.transform(X_va)\n",
        "\n",
        "        # Recency weights on training slice (by bars)\n",
        "        w_tr = compute_recency_weights_by_bars(len(d_tr), TAU_BARS)\n",
        "\n",
        "        # Model config\n",
        "        clf = lgb.LGBMClassifier(\n",
        "            objective=\"binary\",\n",
        "            boosting_type=\"gbdt\",\n",
        "            metric=[\"auc\", \"binary_logloss\"],\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=63,\n",
        "            max_depth=6,\n",
        "            min_child_samples=40,\n",
        "            feature_fraction=0.85,\n",
        "            bagging_fraction=0.85,\n",
        "            bagging_freq=1,\n",
        "            lambda_l1=0.1,\n",
        "            lambda_l2=0.1,\n",
        "            min_gain_to_split=0.0,\n",
        "            n_estimators=4000,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            bagging_seed=RANDOM_STATE,\n",
        "            feature_fraction_seed=RANDOM_STATE,\n",
        "            deterministic=True,\n",
        "        )\n",
        "\n",
        "        clf.fit(\n",
        "            X_trs, y_tr,\n",
        "            sample_weight=w_tr if len(w_tr) == len(y_tr) else None,\n",
        "            eval_set=[(X_vas, y_va)],\n",
        "            eval_metric=\"auc\",\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n",
        "        )\n",
        "\n",
        "        best_iter = int(getattr(clf, \"best_iteration_\", clf.n_estimators))\n",
        "        best_iters.append(best_iter)\n",
        "\n",
        "        p_va = clf.predict_proba(X_vas)[:, 1]\n",
        "        # threshold sweep with guards via helper\n",
        "        best = ev_sweep(p_va, r_va, THRESHOLDS, COST_ROUNDTRIP,\n",
        "                        min_trades=MIN_TRADES_PER_FOLD, ev_floor=EV_FLOOR)\n",
        "\n",
        "        fold_stats.append({\n",
        "            \"fold\": k,\n",
        "            \"auc\": float(clf.best_score_.get(\"valid_0\", {}).get(\"auc\", np.nan)),\n",
        "            \"best_ev\": safe_float(best.get(\"best_ev\")),\n",
        "            \"hit_net\": safe_float(best.get(\"best_hit_net\")),\n",
        "            \"hit_gross\": safe_float(best.get(\"best_hit_gross\")),\n",
        "            \"n_trades\": int(best[\"best_trades\"]),\n",
        "            \"best_theta\": safe_float(best.get(\"best_theta\")),\n",
        "            \"best_iter\": best_iter,\n",
        "            \"p_up_mean\": float(p_va.mean()),\n",
        "            \"p_up_std\": float(p_va.std(ddof=1)),\n",
        "        })\n",
        "        if best.get(\"best_theta\") is not None:\n",
        "            fold_thetas.append(best[\"best_theta\"])\n",
        "\n",
        "    if len(fold_thetas) == 0:\n",
        "        raise RuntimeError(\"No threshold met EV/trade floor and min-trades guard in any fold.\")\n",
        "    else:\n",
        "        theta_deploy = float(np.median(fold_thetas))\n",
        "\n",
        "    best_iter_median = int(np.median(best_iters)) if len(best_iters) else 1000\n",
        "    return theta_deploy, fold_stats, best_iter_median\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 6: FINAL REFIT WITH EARLY STOPPING HOLDOUT, THEN OOS SCORING\n",
        "def refit_and_score_oos(X: np.ndarray,\n",
        "                        y: np.ndarray,\n",
        "                        r: np.ndarray,\n",
        "                        dates: pd.Series,\n",
        "                        cutoff_date: pd.Timestamp,\n",
        "                        theta_deploy: float,\n",
        "                        best_iter_hint: int) -> dict:\n",
        "    \"\"\"Refit on in-sample with an ES holdout, then score OOS and compute metrics.\"\"\"\n",
        "    in_mask = dates.values.astype(\"datetime64[ns]\") <= np.datetime64(cutoff_date)\n",
        "    X_in, y_in = X[in_mask], y[in_mask]\n",
        "\n",
        "    # 90/10 split for early stopping only\n",
        "    cut = int(0.9 * len(X_in))\n",
        "    if cut <= 0:\n",
        "        raise RuntimeError(\"Not enough in-sample data for early-stopping split.\")\n",
        "    X_tr_all, X_es = X_in[:cut], X_in[cut:]\n",
        "    y_tr_all, y_es = y_in[:cut], y_in[cut:]\n",
        "    d_tr_all = pd.to_datetime(dates[in_mask].iloc[:cut])\n",
        "\n",
        "    scaler_final = StandardScaler().fit(X_tr_all)\n",
        "    X_tr_all_s = scaler_final.transform(X_tr_all)\n",
        "    X_es_s = scaler_final.transform(X_es)\n",
        "\n",
        "    # Recency weights for full train (by bars)\n",
        "    w_tr_all = compute_recency_weights_by_bars(len(d_tr_all), TAU_BARS)\n",
        "\n",
        "    clf_final = lgb.LGBMClassifier(\n",
        "        objective=\"binary\",\n",
        "        boosting_type=\"gbdt\",\n",
        "        metric=[\"auc\", \"binary_logloss\"],\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        max_depth=6,\n",
        "        min_child_samples=40,\n",
        "        feature_fraction=0.85,\n",
        "        bagging_fraction=0.85,\n",
        "        bagging_freq=1,\n",
        "        lambda_l1=0.1,\n",
        "        lambda_l2=0.1,\n",
        "        min_gain_to_split=0.0,\n",
        "        n_estimators=max(1000, int(best_iter_hint * 2)),  # large cap; ES will cut down\n",
        "        n_jobs=-1,\n",
        "        verbose=-1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        bagging_seed=RANDOM_STATE,\n",
        "        feature_fraction_seed=RANDOM_STATE,\n",
        "        deterministic=True,\n",
        "    )\n",
        "\n",
        "    clf_final.fit(\n",
        "        X_tr_all_s, y_tr_all,\n",
        "        sample_weight=w_tr_all if len(w_tr_all) == len(y_tr_all) else None,\n",
        "        eval_set=[(X_es_s, y_es)],\n",
        "        eval_metric=\"auc\",\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n",
        "    )\n",
        "\n",
        "    # Determine best iteration from ES, then refit scaler on full in-sample and model with fixed trees\n",
        "    best_n = int(getattr(clf_final, \"best_iteration_\", clf_final.n_estimators))\n",
        "    X_in = X[in_mask]\n",
        "    y_in = y[in_mask]\n",
        "    scaler_final = StandardScaler().fit(X_in)\n",
        "    X_in_s = scaler_final.transform(X_in)\n",
        "\n",
        "    clf_final_refit = lgb.LGBMClassifier(\n",
        "        objective=\"binary\",\n",
        "        boosting_type=\"gbdt\",\n",
        "        metric=[\"auc\", \"binary_logloss\"],\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        max_depth=6,\n",
        "        min_child_samples=40,\n",
        "        feature_fraction=0.85,\n",
        "        bagging_fraction=0.85,\n",
        "        bagging_freq=1,\n",
        "        lambda_l1=0.1,\n",
        "        lambda_l2=0.1,\n",
        "        min_gain_to_split=0.0,\n",
        "        n_estimators=max(50, best_n),\n",
        "        n_jobs=-1,\n",
        "        verbose=-1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        bagging_seed=RANDOM_STATE,\n",
        "        feature_fraction_seed=RANDOM_STATE,\n",
        "        deterministic=True,\n",
        "    )\n",
        "    # Apply recency weights for final refit as well\n",
        "    w_in = compute_recency_weights_by_bars(len(y_in), TAU_BARS)\n",
        "    clf_final_refit.fit(X_in_s, y_in, sample_weight=w_in)\n",
        "\n",
        "    # Score OOS \u2014 now prefer FRONTTEST rows for the OOS window (already merged in load)\n",
        "    oos_mask = dates.values.astype(\"datetime64[ns]\") > np.datetime64(cutoff_date)\n",
        "    X_oos = X[oos_mask]\n",
        "    r_oos = r[oos_mask]\n",
        "    d_oos = pd.to_datetime(dates[oos_mask])\n",
        "\n",
        "    X_oos_s = scaler_final.transform(X_oos) if len(X_oos) else np.empty((0, X_in_s.shape[1]))\n",
        "    p_oos = clf_final_refit.predict_proba(X_oos_s)[:, 1] if len(X_oos_s) else np.array([])\n",
        "\n",
        "    trade = (p_oos >= theta_deploy).astype(int) if len(p_oos) else np.array([])\n",
        "    ev_contrib = trade * (r_oos - COST_ROUNDTRIP) if len(trade) else np.array([])\n",
        "\n",
        "    # Metrics (report both gross and net hit)\n",
        "    trades = int(trade.sum()) if len(trade) else 0\n",
        "    ev_per_trade = float(ev_contrib[trade == 1].mean()) if trades > 0 else float(\"nan\")\n",
        "    hit_gross = float((r_oos[trade == 1] > 0).mean()) if trades > 0 else float(\"nan\")\n",
        "    hit_net = float(((r_oos[trade == 1] - COST_ROUNDTRIP) > 0).mean()) if trades > 0 else float(\"nan\")\n",
        "    trades_per_day = float(trades / len(r_oos)) if len(r_oos) else 0.0\n",
        "\n",
        "    if len(ev_contrib):\n",
        "        cum_log = np.cumsum(ev_contrib)\n",
        "        cum_pct = np.exp(cum_log) - 1.0\n",
        "        # Max drawdown on equity curve in percent space\n",
        "        equity = 1.0 + cum_pct\n",
        "        max_equity = np.maximum.accumulate(equity)\n",
        "        dd = 1.0 - (equity / np.maximum(max_equity, 1e-12))\n",
        "        max_dd = float(np.max(dd))\n",
        "        sharpe_daily = float((ev_contrib.mean() / (ev_contrib.std(ddof=1) + 1e-12)) * math.sqrt(252.0))\n",
        "        cum_log_last = float(cum_log[-1])\n",
        "        cum_pct_last = float(cum_pct[-1])\n",
        "        exposure = float((trade.mean()))\n",
        "        ann_return = float((1.0 + cum_pct_last) ** (252.0 / len(r_oos)) - 1.0) if len(r_oos) else float('nan')\n",
        "        bh_cum = float(np.exp(r_oos.sum()) - 1.0)\n",
        "    else:\n",
        "        max_dd = float(\"nan\")\n",
        "        sharpe_daily = float(\"nan\")\n",
        "        cum_log_last = 0.0\n",
        "        cum_pct_last = 0.0\n",
        "        exposure = 0.0\n",
        "        ann_return = float('nan')\n",
        "        bh_cum = float('nan')\n",
        "\n",
        "    # Save artifacts\n",
        "    joblib.dump(clf_final_refit, MODEL_PATH)\n",
        "    joblib.dump(scaler_final, SCALER_PATH)\n",
        "\n",
        "    # Save per-day OOS predictions\n",
        "    pd.DataFrame({\n",
        "        \"date\": d_oos.astype(str),\n",
        "        \"p_up\": p_oos,\n",
        "        \"trade\": trade,\n",
        "        \"ret_next\": r_oos,\n",
        "        \"ev_contrib\": ev_contrib,\n",
        "    }).to_csv(OOS_PRED_CSV, index=False)\n",
        "\n",
        "    return {\n",
        "        \"oos_start\": str(d_oos.min().date()) if len(d_oos) else None,\n",
        "        \"oos_end\": str(d_oos.max().date()) if len(d_oos) else None,\n",
        "        \"trades\": trades,\n",
        "        \"trades_per_day\": trades_per_day,\n",
        "        \"ev_per_trade\": ev_per_trade,\n",
        "        \"hit_rate_gross\": hit_gross,\n",
        "        \"hit_rate_net\": hit_net,\n",
        "        \"cum_log_pnl\": cum_log_last,\n",
        "        \"cum_pct_pnl\": cum_pct_last,\n",
        "        \"max_drawdown_pct\": max_dd,\n",
        "        \"sharpe_daily\": sharpe_daily,\n",
        "        \"best_iteration_final\": int(getattr(clf_final_refit, \"n_estimators\", 0)),\n",
        "        \"exposure\": exposure,\n",
        "        \"annualized_return\": ann_return,\n",
        "        \"bh_cum_pct\": bh_cum,\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% CELL 7: MAIN ENTRYPOINT \u2014 COMBINED DATA ONLY\n",
        "def load_combined_df() -> pd.DataFrame:\n",
        "    \"\"\"Load combined 1D CSV. Expect exact columns: symbol, timestamp, raw_ohlcv_vec, iso_ohlc, future.\"\"\"\n",
        "    if not COMBINED_1D_CSV_DRIVE.exists():\n",
        "        raise FileNotFoundError(f\"Combined CSV not found at {COMBINED_1D_CSV_DRIVE}\")\n",
        "    df = pd.read_csv(COMBINED_1D_CSV_DRIVE)\n",
        "    required = [\"symbol\", \"timestamp\", \"raw_ohlcv_vec\", \"iso_ohlc\", \"future\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Combined CSV missing required columns: {missing}\")\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load combined dataset only\n",
        "    print(f\"\ud83d\udcc4 Using combined dataset: {COMBINED_1D_CSV_DRIVE}\")\n",
        "    raw_all = load_combined_df()\n",
        "\n",
        "    # Use only the last 6 years of data (by timestamp)\n",
        "    ts_all = pd.to_datetime(raw_all['timestamp'])\n",
        "    last_date = ts_all.max().normalize()\n",
        "    start_date = last_date - pd.DateOffset(years=6)\n",
        "    raw_all = raw_all.loc[ts_all >= start_date].reset_index(drop=True)\n",
        "\n",
        "    # Build features/targets (after 6-year filter)\n",
        "    feat, y_cls, ret_next, dates = build_features_targets(raw_all)\n",
        "\n",
        "    # Cutoff: last 63 trading rows are OOS; everything before is in-sample\n",
        "    if len(dates) <= 63:\n",
        "        raise RuntimeError(\"Not enough data after 6-year filter to allocate 63 OOS rows.\")\n",
        "    cutoff_idx = len(dates) - 63 - 1\n",
        "    cutoff_date = pd.to_datetime(dates.iloc[cutoff_idx]).normalize()\n",
        "    X = feat.values.astype(float)\n",
        "    y = y_cls.values.astype(int)\n",
        "    r = ret_next.values.astype(float)\n",
        "\n",
        "    # Run CV for theta and iteration hint\n",
        "    theta_deploy, fold_stats, best_iter_median = run_time_series_cv(\n",
        "        X, y, r, dates, cutoff_date\n",
        "    )\n",
        "\n",
        "    # Final refit & OOS\n",
        "    oos_stats = refit_and_score_oos(\n",
        "        X, y, r, dates, cutoff_date, theta_deploy, best_iter_median\n",
        "    )\n",
        "\n",
        "    # Save threshold & metrics JSONs + manifest\n",
        "    with open(THRESHOLD_JSON, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"theta\": float(theta_deploy),\n",
        "            \"cost_bps\": 6,\n",
        "            \"fold_stats\": fold_stats,\n",
        "            \"theta_selection\": \"median_of_fold_bests\",\n",
        "        }, f, indent=2)\n",
        "\n",
        "    with open(METRICS_JSON, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"in_sample\": {\n",
        "                \"folds\": fold_stats,\n",
        "                \"theta_deploy\": float(theta_deploy),\n",
        "            },\n",
        "            \"oos\": {\n",
        "                **oos_stats\n",
        "            },\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Persist feature contract and config manifest\n",
        "    feature_names = list(feat.columns)\n",
        "    manifest = {\n",
        "        \"model_type\": \"LightGBMClassifier\",\n",
        "        \"random_state\": RANDOM_STATE,\n",
        "        \"cost_roundtrip\": COST_ROUNDTRIP,\n",
        "        \"cost_mode\": \"scalar\",  # upgrade to per_row in future\n",
        "        \"tau_bars\": TAU_BARS,\n",
        "        \"threshold_sweep\": [float(THRESHOLDS.min()), float(THRESHOLDS.max()), 0.01],\n",
        "        \"ev_floor\": EV_FLOOR,\n",
        "        \"min_trades_per_fold\": MIN_TRADES_PER_FOLD,\n",
        "        \"feature_names\": feature_names,\n",
        "        \"cutoff_date\": str(pd.to_datetime(cutoff_date).date()),\n",
        "        \"theta_deploy\": float(theta_deploy),\n",
        "        \"guards\": {\n",
        "            \"ev_floor\": EV_FLOOR,\n",
        "            \"min_trades_per_fold\": MIN_TRADES_PER_FOLD\n",
        "        },\n",
        "        \"artifact_paths\": {\n",
        "            \"model\": str(MODEL_PATH),\n",
        "            \"scaler\": str(SCALER_PATH),\n",
        "            \"threshold_json\": str(THRESHOLD_JSON),\n",
        "            \"metrics_json\": str(METRICS_JSON),\n",
        "            \"oos_preds_csv\": str(OOS_PRED_CSV),\n",
        "        },\n",
        "        \"lgbm_params\": {\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"num_leaves\": 63,\n",
        "            \"max_depth\": 6,\n",
        "            \"min_child_samples\": 40,\n",
        "            \"feature_fraction\": 0.85,\n",
        "            \"bagging_fraction\": 0.85,\n",
        "            \"bagging_freq\": 1,\n",
        "            \"lambda_l1\": 0.1,\n",
        "            \"lambda_l2\": 0.1,\n",
        "        },\n",
        "        \"execution_convention\": \"enter close_t, exit close_t+1 (log returns)\",\n",
        "    }\n",
        "    with open((ARTIFACT_DIR if ARTIFACT_DIR else REPORTS_DIR) / \"model_manifest.json\", \"w\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n",
        "    # Minimal console output\n",
        "    print(\"\\n=== LightGBM 1D v0 \u2014 COMPLETE ===\")\n",
        "    print(f\"Theta (deploy): {theta_deploy:.2f}\")\n",
        "    print(f\"OOS window: {oos_stats['oos_start']} \u2192 {oos_stats['oos_end']}\")\n",
        "    hit_g = oos_stats.get('hit_rate_gross', float('nan'))\n",
        "    hit_n = oos_stats.get('hit_rate_net', float('nan'))\n",
        "    print(f\"Trades: {oos_stats['trades']}  | EV/trade: {oos_stats['ev_per_trade']:.6f}  | Hit(gross/net): {hit_g:.3f}/{hit_n:.3f}\")\n",
        "    print(f\"Cum % PnL: {oos_stats['cum_pct_pnl']*100:.2f}%  | MaxDD: {oos_stats['max_drawdown_pct']*100:.2f}%  | Sharpe(d): {oos_stats['sharpe_daily']:.2f}\")\n",
        "    print(f\"Exposure: {oos_stats['exposure']:.3f}  | Annualized Return: {oos_stats['annualized_return']*100:.2f}%  | Baseline(BH OOS): {oos_stats['bh_cum_pct']*100:.2f}%\")\n",
        "    print(\"Artifacts saved:\")\n",
        "    print(f\"- Model:   {MODEL_PATH}\")\n",
        "    print(f\"- Scaler:  {SCALER_PATH}\")\n",
        "    print(f\"- Reports: {THRESHOLD_JSON}, {METRICS_JSON}\")\n",
        "    print(f\"- Preds:   {OOS_PRED_CSV}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}